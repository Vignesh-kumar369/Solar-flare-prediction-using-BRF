{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "651c3f3b-4733-4b94-8ee2-dc2369dc7c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing chunk: 2014-06-14 ---\n",
      "  Attempt 1 of 3...\n",
      "    Querying JSOC for SHARP data...\n",
      "    Saved 1828 SHARP records.\n",
      "    Querying for SWPC GOES flare catalog...\n",
      "    Saved 5 flare events.\n",
      "\n",
      "--- Processing chunk: 2014-06-15 ---\n",
      "  Attempt 1 of 3...\n",
      "    Querying JSOC for SHARP data...\n",
      "    Saved 1518 SHARP records.\n",
      "    Querying for SWPC GOES flare catalog...\n",
      "    Saved 9 flare events.\n",
      "\n",
      "--- Processing chunk: 2014-06-16 ---\n",
      "  Attempt 1 of 3...\n",
      "    Querying JSOC for SHARP data...\n",
      "    Saved 1275 SHARP records.\n",
      "    Querying for SWPC GOES flare catalog...\n",
      "    Saved 10 flare events.\n",
      "\n",
      "✅ Raw data collection complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from sunpy.net import Fido, attrs as a\n",
    "from sunpy.net.jsoc import JSOCClient\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "from astropy.utils.data import conf as astropy_conf\n",
    "\n",
    "# =========================\n",
    "# 1. CONFIGURATION\n",
    "# =========================\n",
    "astropy_conf.remote_timeout = 90.0\n",
    "\n",
    "EMAIL_FOR_JSOC = 'your_email@gmail.com'\n",
    "TIME_RANGE_START = '2014-06-14T00:00:00'\n",
    "TIME_RANGE_END = '2014-06-16T23:59:59'\n",
    "\n",
    "SHARP_DIR_OUTPUT = 'raw_sharp_data_parquet'\n",
    "FLARE_DIR_OUTPUT = 'raw_flare_data_parquet'\n",
    "os.makedirs(SHARP_DIR_OUTPUT, exist_ok=True)\n",
    "os.makedirs(FLARE_DIR_OUTPUT, exist_ok=True)\n",
    "\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "SHARP_FEATURES = [\n",
    "    'USFLUX', 'MEANGAM', 'MEANGBT', 'MEANGBZ', 'MEANGBH', 'TOTUSJH',\n",
    "    'TOTPOT', 'MEANPOT', 'TOTUSJZ', 'SAVNCPP', 'ABSNJZH', 'AREA_ACR',\n",
    "    'MEANJZH', 'R_VALUE', 'LAT_FWT', 'TOTUSI', 'TOTBSQ', 'TOTFX', 'TOTFY',\n",
    "    'TOTFZ', 'EPSX', 'EPSY', 'EPSZ', 'MEANALP', 'MEANSHR'\n",
    "]\n",
    "\n",
    "# =========================\n",
    "# 2. DATA COLLECTION\n",
    "# =========================\n",
    "jsoc_client = JSOCClient()\n",
    "start_time = pd.to_datetime(TIME_RANGE_START)\n",
    "end_time = pd.to_datetime(TIME_RANGE_END)\n",
    "\n",
    "current_start = start_time\n",
    "while current_start < end_time:\n",
    "    current_end = current_start + DateOffset(days=1)\n",
    "    if current_end > end_time:\n",
    "        current_end = end_time\n",
    "\n",
    "    print(f\"\\n--- Processing chunk: {current_start.date()} ---\")\n",
    "    \n",
    "    sharp_output_file = f\"{SHARP_DIR_OUTPUT}/{current_start.date()}.parquet\"\n",
    "    flare_output_file = f\"{FLARE_DIR_OUTPUT}/{current_start.date()}.parquet\"\n",
    "\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            print(f\"  Attempt {attempt + 1} of {MAX_RETRIES}...\")\n",
    "            start_str = current_start.isoformat()\n",
    "            end_str = current_end.isoformat()\n",
    "            \n",
    "            # --- Fetch SHARP data ---\n",
    "            if not os.path.exists(sharp_output_file):\n",
    "                print(\"    Querying JSOC for SHARP data...\")\n",
    "                sharp_response = jsoc_client.search(\n",
    "                    a.Time(start_str, end_str),\n",
    "                    a.jsoc.Series('hmi.sharp_720s'),\n",
    "                    a.jsoc.Notify(EMAIL_FOR_JSOC)\n",
    "                )\n",
    "\n",
    "                if len(sharp_response) > 0:\n",
    "                    sharp_df_chunk = sharp_response.to_pandas()\n",
    "                    identifiers = ['T_REC', 'HARPNUM']\n",
    "                    features_that_exist = [col for col in SHARP_FEATURES if col in sharp_df_chunk.columns]\n",
    "                    sharp_df_filtered = sharp_df_chunk[identifiers + features_that_exist]\n",
    "                    sharp_df_filtered.to_parquet(sharp_output_file)\n",
    "                    print(f\"    Saved {len(sharp_df_filtered)} SHARP records.\")\n",
    "                else:\n",
    "                    print(\"    No SHARP data found for this day.\")\n",
    "            else:\n",
    "                print(\"    SHARP data already exists. Skipping SHARP.\")\n",
    "\n",
    "            # --- Fetch Flare data ---\n",
    "            if not os.path.exists(flare_output_file):\n",
    "                print(\"    Querying for SWPC GOES flare catalog...\")\n",
    "                flare_response = Fido.search(\n",
    "                    a.Time(start_str, end_str),\n",
    "                    a.hek.EventType('FL'),\n",
    "                    a.hek.FRM.Name == 'SWPC'\n",
    "                )\n",
    "\n",
    "                if flare_response:\n",
    "                    flare_table = flare_response[0]\n",
    "                    names = [n for n in flare_table.colnames if len(flare_table[n].shape) <= 1]\n",
    "                    flare_table = flare_table[names]\n",
    "\n",
    "                    flares_df_chunk = flare_table.to_pandas()\n",
    "\n",
    "                    keep_cols = [c for c in ['event_starttime', 'fl_goescls', 'ar_noaanum'] if c in flares_df_chunk.columns]\n",
    "                    flares_df_chunk = flares_df_chunk[keep_cols]\n",
    "\n",
    "                    if 'event_starttime' in flares_df_chunk.columns:\n",
    "                        flares_df_chunk['event_starttime'] = pd.to_datetime(\n",
    "                            flares_df_chunk['event_starttime'], errors='coerce'\n",
    "                        )\n",
    "\n",
    "                    flares_df_chunk.to_parquet(flare_output_file)\n",
    "                    print(f\"    Saved {len(flares_df_chunk)} flare events.\")\n",
    "                else:\n",
    "                    print(\"    No flare data found for this day.\")\n",
    "            else:\n",
    "                print(\"    Flare data already exists. Skipping flares.\")\n",
    "\n",
    "            break # Success\n",
    "\n",
    "        except Exception as e:\n",
    "            if '[status=6]' in str(e):\n",
    "                print(\"    Server response: No data found for this day.\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"    Attempt {attempt + 1} failed with error: {e}\")\n",
    "                if attempt < MAX_RETRIES - 1:\n",
    "                    wait_time = (attempt + 1) * 15\n",
    "                    print(f\"    Waiting {wait_time} seconds before retrying...\")\n",
    "                    time.sleep(wait_time)\n",
    "    \n",
    "    current_start = current_end\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"\\n✅ Raw data collection complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a7cba0f-4b9a-433c-914b-6c883e86d46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading all raw data chunks ---\n",
      "Loaded 965310 SHARP records.\n",
      "Loaded 4380 flare records.\n",
      "\n",
      "--- Step 2: Cleaning and Preparing Data ---\n",
      "All data cleaned and sorted.\n",
      "\n",
      "--- Step 3: Labeling SHARP data using a 24-hour prediction window ---\n",
      "\n",
      "--- Step 4: Saving final labeled dataset ---\n",
      "\n",
      "--- Dataset Summary ---\n",
      "Total SHARP records: 957265\n",
      "\n",
      "Binary label counts ('flare'):\n",
      "flare\n",
      "0    957265\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Multi-class label counts ('classification'):\n",
      "classification\n",
      "Non-flare    957265\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✅ Dataset saved to final_labeled_solar_dataset.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from datetime import timedelta\n",
    "\n",
    "# =========================\n",
    "# CONFIGURATION\n",
    "# =========================\n",
    "SHARP_DIR_INPUT = 'raw_sharp_data_parquet'\n",
    "FLARE_DIR_INPUT = 'raw_flare_data_parquet'\n",
    "FINAL_OUTPUT_FILE = 'final_labeled_solar_dataset.parquet'\n",
    "PREDICTION_WINDOW_HOURS = 24\n",
    "\n",
    "# =========================\n",
    "# 1. LOAD AND COMBINE RAW DATA\n",
    "# =========================\n",
    "print(\"--- Step 1: Loading all raw data chunks ---\")\n",
    "sharp_files = glob.glob(f'{SHARP_DIR_INPUT}/*.parquet')\n",
    "flare_files = glob.glob(f'{FLARE_DIR_INPUT}/*.parquet')\n",
    "\n",
    "if not sharp_files:\n",
    "    raise FileNotFoundError(f\"No SHARP files found in '{SHARP_DIR_INPUT}' directory.\")\n",
    "\n",
    "sharp_df = pd.concat([pd.read_parquet(f) for f in sharp_files], ignore_index=True)\n",
    "print(f\"Loaded {len(sharp_df)} SHARP records.\")\n",
    "\n",
    "if flare_files:\n",
    "    flares_df = pd.concat([pd.read_parquet(f) for f in flare_files], ignore_index=True)\n",
    "    print(f\"Loaded {len(flares_df)} flare records.\")\n",
    "else:\n",
    "    flares_df = pd.DataFrame()\n",
    "    print(\"No flare data files found.\")\n",
    "\n",
    "# =========================\n",
    "# 2. CLEAN AND PREPARE DATA\n",
    "# =========================\n",
    "print(\"\\n--- Step 2: Cleaning and Preparing Data ---\")\n",
    "\n",
    "# --- Fix T_REC datetime ---\n",
    "sharp_df['T_REC'] = (\n",
    "    sharp_df['T_REC']\n",
    "    .astype(str)\n",
    "    .str.replace('_TAI', '', regex=False)   # remove trailing _TAI\n",
    ")\n",
    "sharp_df['T_REC'] = pd.to_datetime(\n",
    "    sharp_df['T_REC'], \n",
    "    format='%Y.%m.%d_%H:%M:%S', \n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "sharp_df.sort_values(by=['HARPNUM', 'T_REC'], inplace=True)\n",
    "sharp_df.drop_duplicates(subset=['HARPNUM', 'T_REC'], inplace=True)\n",
    "sharp_df['HARPNUM'] = sharp_df['HARPNUM'].astype(int)\n",
    "\n",
    "# Initialize label columns\n",
    "sharp_df['classification'] = 'Non-flare'\n",
    "sharp_df['flare'] = 0\n",
    "\n",
    "# Clean flare data (if available)\n",
    "if not flares_df.empty:\n",
    "    flares_df['event_starttime'] = pd.to_datetime(flares_df['event_starttime'], errors='coerce')\n",
    "    flares_df['ar_noaanum'] = pd.to_numeric(flares_df['ar_noaanum'], errors='coerce')\n",
    "    flares_df.dropna(subset=['event_starttime', 'ar_noaanum', 'fl_goescls'], inplace=True)\n",
    "    flares_df = flares_df[flares_df['ar_noaanum'] > 0].copy()\n",
    "    flares_df['ar_noaanum'] = flares_df['ar_noaanum'].astype(int)\n",
    "    flares_df.sort_values('event_starttime', inplace=True)\n",
    "    print(\"All data cleaned and sorted.\")\n",
    "\n",
    "# =========================\n",
    "# 3. CREATE LABELS\n",
    "# =========================\n",
    "if not flares_df.empty:\n",
    "    print(f\"\\n--- Step 3: Labeling SHARP data using a {PREDICTION_WINDOW_HOURS}-hour prediction window ---\")\n",
    "    prediction_window = pd.Timedelta(hours=PREDICTION_WINDOW_HOURS)\n",
    "    flare_classes = ['X', 'M', 'C', 'B']  # strongest to weakest\n",
    "\n",
    "    for f_class in flare_classes:\n",
    "        class_flares = flares_df[flares_df['fl_goescls'].str.startswith(f_class, na=False)]\n",
    "        for _, flare in class_flares.iterrows():\n",
    "            mask = (\n",
    "                (sharp_df['HARPNUM'] == flare['ar_noaanum']) &\n",
    "                (sharp_df['T_REC'] >= flare['event_starttime'] - prediction_window) &\n",
    "                (sharp_df['T_REC'] < flare['event_starttime'])\n",
    "            )\n",
    "            sharp_df.loc[mask, 'classification'] = f_class\n",
    "\n",
    "    # Binary flare label: 1 if X or M flare, else 0\n",
    "    sharp_df['flare'] = sharp_df['classification'].isin(['X', 'M']).astype(int)\n",
    "\n",
    "# =========================\n",
    "# 4. SAVE FINAL DATASET\n",
    "# =========================\n",
    "print(\"\\n--- Step 4: Saving final labeled dataset ---\")\n",
    "sharp_df.to_parquet(FINAL_OUTPUT_FILE, index=False)\n",
    "\n",
    "# =========================\n",
    "# 5. SUMMARY\n",
    "# =========================\n",
    "print(\"\\n--- Dataset Summary ---\")\n",
    "print(f\"Total SHARP records: {len(sharp_df)}\")\n",
    "print(\"\\nBinary label counts ('flare'):\")\n",
    "print(sharp_df['flare'].value_counts())\n",
    "print(\"\\nMulti-class label counts ('classification'):\")\n",
    "print(sharp_df['classification'].value_counts())\n",
    "print(f\"\\n✅ Dataset saved to {FINAL_OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b448c9ca-3d64-418a-a23e-5e6592ae8d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading all raw data chunks ---\n",
      "Loaded 965310 SHARP records.\n",
      "Loaded 4380 flare records.\n",
      "\n",
      "--- Step 2: Cleaning and Preparing Data ---\n",
      "Cleaning flare data...\n",
      "All data cleaned and sorted.\n",
      "\n",
      "--- Step 3: Labeling SHARP data using a 24-hour prediction window ---\n",
      "\n",
      "--- Step 4: Saving final labeled dataset ---\n",
      "\n",
      "--- Dataset Summary ---\n",
      "Total SHARP records: 957265\n",
      "\n",
      "Binary label counts ('flare'):\n",
      "flare\n",
      "0    957265\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Multi-class label counts ('classification'):\n",
      "classification\n",
      "Non-flare    957265\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✅ Dataset saved to final_labeled_solar_dataset.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from datetime import timedelta\n",
    "\n",
    "# =========================\n",
    "# CONFIGURATION\n",
    "# =========================\n",
    "SHARP_DIR_INPUT = 'raw_sharp_data_parquet'\n",
    "FLARE_DIR_INPUT = 'raw_flare_data_parquet'\n",
    "FINAL_OUTPUT_FILE = 'final_labeled_solar_dataset.parquet'\n",
    "PREDICTION_WINDOW_HOURS = 24\n",
    "\n",
    "# =========================\n",
    "# 1. LOAD AND COMBINE RAW DATA\n",
    "# =========================\n",
    "print(\"--- Step 1: Loading all raw data chunks ---\")\n",
    "sharp_files = glob.glob(f'{SHARP_DIR_INPUT}/*.parquet')\n",
    "flare_files = glob.glob(f'{FLARE_DIR_INPUT}/*.parquet')\n",
    "\n",
    "if not sharp_files:\n",
    "    raise FileNotFoundError(f\"No SHARP files found in '{SHARP_DIR_INPUT}' directory.\")\n",
    "\n",
    "sharp_df = pd.concat([pd.read_parquet(f) for f in sharp_files], ignore_index=True)\n",
    "print(f\"Loaded {len(sharp_df)} SHARP records.\")\n",
    "\n",
    "if flare_files:\n",
    "    flares_df = pd.concat([pd.read_parquet(f) for f in flare_files], ignore_index=True)\n",
    "    print(f\"Loaded {len(flares_df)} flare records.\")\n",
    "else:\n",
    "    flares_df = pd.DataFrame()\n",
    "    print(\"No flare data files found.\")\n",
    "\n",
    "# =========================\n",
    "# 2. CLEAN AND PREPARE DATA\n",
    "# =========================\n",
    "print(\"\\n--- Step 2: Cleaning and Preparing Data ---\")\n",
    "\n",
    "# --- Clean SHARP data ---\n",
    "sharp_df['T_REC'] = (\n",
    "    sharp_df['T_REC']\n",
    "    .astype(str)\n",
    "    .str.replace('_TAI', '', regex=False)\n",
    ")\n",
    "sharp_df['T_REC'] = pd.to_datetime(\n",
    "    sharp_df['T_REC'], \n",
    "    format='%Y.%m.%d_%H:%M:%S', \n",
    "    errors='coerce'\n",
    ")\n",
    "sharp_df.dropna(subset=['T_REC'], inplace=True)\n",
    "sharp_df.sort_values(by=['HARPNUM', 'T_REC'], inplace=True)\n",
    "sharp_df.drop_duplicates(subset=['HARPNUM', 'T_REC'], inplace=True)\n",
    "sharp_df['HARPNUM'] = sharp_df['HARPNUM'].astype(int)\n",
    "\n",
    "# Initialize label columns\n",
    "sharp_df['classification'] = 'Non-flare'\n",
    "sharp_df['flare'] = 0\n",
    "\n",
    "# Clean flare data (if available)\n",
    "if not flares_df.empty:\n",
    "    print(\"Cleaning flare data...\")\n",
    "    flares_df['event_starttime'] = pd.to_datetime(flares_df['event_starttime'], errors='coerce')\n",
    "    flares_df['ar_noaanum'] = pd.to_numeric(flares_df['ar_noaanum'], errors='coerce')\n",
    "    flares_df.dropna(subset=['event_starttime', 'ar_noaanum', 'fl_goescls'], inplace=True)\n",
    "    \n",
    "    # --- THE FIX: Filter out flares with no valid AR number (like 0) ---\n",
    "    flares_df = flares_df[flares_df['ar_noaanum'] > 0].copy()\n",
    "    \n",
    "    flares_df['ar_noaanum'] = flares_df['ar_noaanum'].astype(int)\n",
    "    flares_df.sort_values('event_starttime', inplace=True)\n",
    "    print(\"All data cleaned and sorted.\")\n",
    "\n",
    "# =========================\n",
    "# 3. CREATE LABELS\n",
    "# =========================\n",
    "# (The rest of your script is correct and remains the same)\n",
    "if not flares_df.empty:\n",
    "    print(f\"\\n--- Step 3: Labeling SHARP data using a {PREDICTION_WINDOW_HOURS}-hour prediction window ---\")\n",
    "    prediction_window = pd.Timedelta(hours=PREDICTION_WINDOW_HOURS)\n",
    "    flare_classes = ['X', 'M', 'C', 'B']\n",
    "\n",
    "    for f_class in flare_classes:\n",
    "        class_flares = flares_df[flares_df['fl_goescls'].str.startswith(f_class, na=False)]\n",
    "        for _, flare in class_flares.iterrows():\n",
    "            mask = (\n",
    "                (sharp_df['HARPNUM'] == flare['ar_noaanum']) &\n",
    "                (sharp_df['T_REC'] >= flare['event_starttime'] - prediction_window) &\n",
    "                (sharp_df['T_REC'] < flare['event_starttime'])\n",
    "            )\n",
    "            sharp_df.loc[mask, 'classification'] = f_class\n",
    "\n",
    "    sharp_df['flare'] = sharp_df['classification'].isin(['X', 'M']).astype(int)\n",
    "\n",
    "# =========================\n",
    "# 4. SAVE FINAL DATASET\n",
    "# =========================\n",
    "print(\"\\n--- Step 4: Saving final labeled dataset ---\")\n",
    "sharp_df.to_parquet(FINAL_OUTPUT_FILE, index=False)\n",
    "\n",
    "# =========================\n",
    "# 5. SUMMARY\n",
    "# =========================\n",
    "print(\"\\n--- Dataset Summary ---\")\n",
    "print(f\"Total SHARP records: {len(sharp_df)}\")\n",
    "print(\"\\nBinary label counts ('flare'):\")\n",
    "print(sharp_df['flare'].value_counts())\n",
    "print(\"\\nMulti-class label counts ('classification'):\")\n",
    "print(sharp_df['classification'].value_counts())\n",
    "print(f\"\\n✅ Dataset saved to {FINAL_OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8678729-e4d1-48be-b558-9f6a911b88ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading all raw data chunks ---\n",
      "Loaded 965310 SHARP records.\n",
      "Loaded 4380 flare records.\n",
      "\n",
      "--- Step 2: Cleaning and Preparing Data ---\n",
      "All data cleaned and sorted.\n",
      "\n",
      "--- Step 3: Running Diagnostics on a Single Flare ---\n",
      "\n",
      "[1] Details of the first powerful flare found:\n",
      "event_starttime    2013-01-05 09:26:00\n",
      "fl_goescls                        M1.7\n",
      "ar_noaanum                       11652\n",
      "\n",
      "[2] Checking for SHARP data for HARPNUM 11652 within the time window:\n",
      "    Start of window: 2013-01-04 09:26:00\n",
      "    End of window:   2013-01-05 09:26:00\n",
      "\n",
      ">> DIAGNOSTIC RESULT: Found a flare from AR 11652, but there is NO SHARP data at all for this HARPNUM.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from datetime import timedelta\n",
    "\n",
    "# =========================\n",
    "# CONFIGURATION\n",
    "# =========================\n",
    "SHARP_DIR_INPUT = 'raw_sharp_data_parquet'\n",
    "FLARE_DIR_INPUT = 'raw_flare_data_parquet'\n",
    "PREDICTION_WINDOW_HOURS = 24\n",
    "\n",
    "# =========================\n",
    "# 1. LOAD AND COMBINE RAW DATA\n",
    "# =========================\n",
    "print(\"--- Step 1: Loading all raw data chunks ---\")\n",
    "sharp_files = glob.glob(f'{SHARP_DIR_INPUT}/*.parquet')\n",
    "flare_files = glob.glob(f'{FLARE_DIR_INPUT}/*.parquet')\n",
    "\n",
    "if not sharp_files:\n",
    "    raise FileNotFoundError(f\"No SHARP files found in '{SHARP_DIR_INPUT}' directory.\")\n",
    "\n",
    "sharp_df = pd.concat([pd.read_parquet(f) for f in sharp_files], ignore_index=True)\n",
    "print(f\"Loaded {len(sharp_df)} SHARP records.\")\n",
    "\n",
    "if flare_files:\n",
    "    flares_df = pd.concat([pd.read_parquet(f) for f in flare_files], ignore_index=True)\n",
    "    print(f\"Loaded {len(flares_df)} flare records.\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"No flare files found in '{FLARE_DIR_INPUT}' directory.\")\n",
    "\n",
    "# =========================\n",
    "# 2. CLEAN AND PREPARE DATA\n",
    "# =========================\n",
    "print(\"\\n--- Step 2: Cleaning and Preparing Data ---\")\n",
    "sharp_df['T_REC'] = pd.to_datetime(sharp_df['T_REC'], format=\"%Y.%m.%d_%H:%M:%S_TAI\")\n",
    "sharp_df['HARPNUM'] = sharp_df['HARPNUM'].astype(int)\n",
    "\n",
    "flares_df['event_starttime'] = pd.to_datetime(flares_df['event_starttime'], errors='coerce')\n",
    "flares_df.dropna(subset=['event_starttime', 'ar_noaanum', 'fl_goescls'], inplace=True)\n",
    "flares_df['ar_noaanum'] = pd.to_numeric(flares_df['ar_noaanum'], errors='coerce')\n",
    "flares_df = flares_df[flares_df['ar_noaanum'] > 0].copy()\n",
    "flares_df['ar_noaanum'] = flares_df['ar_noaanum'].astype(int)\n",
    "flares_df.sort_values('event_starttime', inplace=True)\n",
    "print(\"All data cleaned and sorted.\")\n",
    "\n",
    "# ===================================================================\n",
    "# 3. DIAGNOSTIC BLOCK\n",
    "# ===================================================================\n",
    "print(\"\\n--- Step 3: Running Diagnostics on a Single Flare ---\")\n",
    "\n",
    "# Find the first powerful flare (M or X class)\n",
    "powerful_flares = flares_df[flares_df['fl_goescls'].str.startswith(('M', 'X'), na=False)]\n",
    "\n",
    "if powerful_flares.empty:\n",
    "    print(\"\\n>> DIAGNOSTIC RESULT: No M-class or X-class flares were found in your flare data files.\")\n",
    "    print(\">> This is the reason no labels are being created.\")\n",
    "else:\n",
    "    # Get the details of the first powerful flare\n",
    "    first_flare = powerful_flares.iloc[0]\n",
    "    flare_time = first_flare['event_starttime']\n",
    "    ar_num = first_flare['ar_noaanum']\n",
    "    \n",
    "    print(\"\\n[1] Details of the first powerful flare found:\")\n",
    "    print(first_flare.to_string())\n",
    "    \n",
    "    # Define the 24-hour window *before* this flare\n",
    "    prediction_window = pd.Timedelta(hours=PREDICTION_WINDOW_HOURS)\n",
    "    start_window = flare_time - prediction_window\n",
    "    end_window = flare_time\n",
    "    \n",
    "    print(f\"\\n[2] Checking for SHARP data for HARPNUM {ar_num} within the time window:\")\n",
    "    print(f\"    Start of window: {start_window}\")\n",
    "    print(f\"    End of window:   {end_window}\")\n",
    "    \n",
    "    # Find all SHARP records that match the Active Region number\n",
    "    sharp_subset = sharp_df[sharp_df['HARPNUM'] == ar_num]\n",
    "    \n",
    "    if sharp_subset.empty:\n",
    "        print(f\"\\n>> DIAGNOSTIC RESULT: Found a flare from AR {ar_num}, but there is NO SHARP data at all for this HARPNUM.\")\n",
    "    else:\n",
    "        print(f\"\\n[3] Found {len(sharp_subset)} total SHARP records for HARPNUM {ar_num}.\")\n",
    "        print(f\"    Earliest SHARP record for this AR is at: {sharp_subset['T_REC'].min()}\")\n",
    "        print(f\"    Latest SHARP record for this AR is at:   {sharp_subset['T_REC'].max()}\")\n",
    "        \n",
    "        # Now, find the records within the specific 24-hour window\n",
    "        potential_matches = sharp_subset[\n",
    "            (sharp_subset['T_REC'] >= start_window) & (sharp_subset['T_REC'] < end_window)\n",
    "        ]\n",
    "        \n",
    "        print(\"\\n[4] Searching for SHARP records within the 24-hour pre-flare window...\")\n",
    "        if potential_matches.empty:\n",
    "            print(\"\\n>> DIAGNOSTIC RESULT: Found SHARP data for this AR, but NONE of it falls within the 24-hour window before the flare.\")\n",
    "            print(\">> This could be due to a data gap right before the flare.\")\n",
    "        else:\n",
    "            print(f\"\\n>> DIAGNOSTIC SUCCESS: Found {len(potential_matches)} matching SHARP records that should be labeled!\")\n",
    "            print(\">> First 5 matching records:\")\n",
    "            print(potential_matches.head())\n",
    "            \n",
    "# ==================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b92487a-69e5-4a02-878c-5577b6ed4b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The actual column names in 'raw_sharp_data_parquet\\2014-01-01.parquet' are:\n",
      "Index(['T_REC', 'HARPNUM', 'USFLUX', 'MEANGAM', 'MEANGBT', 'MEANGBZ',\n",
      "       'MEANGBH', 'TOTUSJH', 'TOTPOT', 'MEANPOT', 'TOTUSJZ', 'SAVNCPP',\n",
      "       'ABSNJZH', 'AREA_ACR', 'MEANJZH', 'R_VALUE', 'LAT_FWT', 'MEANALP',\n",
      "       'MEANSHR'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# The folder where your daily SHARP files are saved\n",
    "SHARP_DIR = 'raw_sharp_data_parquet'\n",
    "# The specific daily file you want to inspect\n",
    "FILE_TO_INSPECT = '2014-01-01.parquet' \n",
    "# ---------------------\n",
    "\n",
    "# Construct the full path to the file\n",
    "file_path = os.path.join(SHARP_DIR, FILE_TO_INSPECT)\n",
    "\n",
    "try:\n",
    "    # Load the Parquet file into a pandas DataFrame\n",
    "    # Note: Parquet is read differently than CSV, so we don't need nrows.\n",
    "    # We just load the whole (small) daily file.\n",
    "    sharp_df = pd.read_parquet(file_path)\n",
    "\n",
    "    print(f\"The actual column names in '{file_path}' are:\")\n",
    "    print(sharp_df.columns)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found.\")\n",
    "    print(\"Please make sure you have run the data collector script and the file exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97eba9ec-ec1f-42ac-a715-0556fb202d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Verifying data in 'raw_flare_data_parquet' ---\n",
      "\n",
      "Step 1: Successfully loaded a total of 4380 raw flare records.\n",
      "Step 2: After dropping rows with any missing values, 4380 records remain (removed 0).\n",
      "Step 3: After removing non-numeric AR numbers, 4380 records remain (removed 0).\n",
      "Step 4: After filtering for positive AR numbers (> 0), 3846 records remain (removed 534).\n",
      "\n",
      "--- VERIFICATION COMPLETE ---\n",
      "RESULT: ✅ Found 3846 usable flare records with valid Active Region numbers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# --- Configuration ---\n",
    "FLARE_DIR_INPUT = 'raw_flare_data_parquet'\n",
    "# ---------------------\n",
    "\n",
    "print(f\"--- Verifying data in '{FLARE_DIR_INPUT}' ---\")\n",
    "\n",
    "# 1. Load all raw flare data from the folder\n",
    "try:\n",
    "    flare_files = glob.glob(f'{FLARE_DIR_INPUT}/*.parquet')\n",
    "    if not flare_files:\n",
    "        raise FileNotFoundError\n",
    "    flares_df = pd.concat([pd.read_parquet(f) for f in flare_files], ignore_index=True)\n",
    "    print(f\"\\nStep 1: Successfully loaded a total of {len(flares_df)} raw flare records.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: No files found in '{FLARE_DIR_INPUT}'. Please run the collector for the 2013-2014 period first.\")\n",
    "    exit()\n",
    "\n",
    "# 2. Clean the data step-by-step and report the counts\n",
    "\n",
    "# Initial cleaning for missing values in key columns\n",
    "initial_count = len(flares_df)\n",
    "flares_df.dropna(subset=['event_starttime', 'ar_noaanum', 'fl_goescls'], inplace=True)\n",
    "print(f\"Step 2: After dropping rows with any missing values, {len(flares_df)} records remain (removed {initial_count - len(flares_df)}).\")\n",
    "\n",
    "# Convert ar_noaanum to a numeric type, marking non-numbers as NaN\n",
    "flares_df['ar_noaanum'] = pd.to_numeric(flares_df['ar_noaanum'], errors='coerce')\n",
    "initial_count = len(flares_df)\n",
    "flares_df.dropna(subset=['ar_noaanum'], inplace=True) # Drop the new NaNs\n",
    "print(f\"Step 3: After removing non-numeric AR numbers, {len(flares_df)} records remain (removed {initial_count - len(flares_df)}).\")\n",
    "\n",
    "# --- THIS IS THE CRITICAL STEP ---\n",
    "# Filter for only the flares with a valid, positive Active Region number.\n",
    "# \"Orphan\" flares often have an AR number of 0 or a negative placeholder.\n",
    "initial_count = len(flares_df)\n",
    "flares_df_cleaned = flares_df[flares_df['ar_noaanum'] > 0]\n",
    "print(f\"Step 4: After filtering for positive AR numbers (> 0), {len(flares_df_cleaned)} records remain (removed {initial_count - len(flares_df_cleaned)}).\")\n",
    "# ---------------------------------\n",
    "\n",
    "print(\"\\n--- VERIFICATION COMPLETE ---\")\n",
    "if len(flares_df_cleaned) == 0:\n",
    "    print(\"RESULT: ❌ Confirmed. The dataset contains 0 flares with a valid, linkable Active Region number.\")\n",
    "else:\n",
    "    print(f\"RESULT: ✅ Found {len(flares_df_cleaned)} usable flare records with valid Active Region numbers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "908e22da-0271-4b13-adb5-c175595366eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 730 daily SHARP data files.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "sharp_files = glob.glob('raw_sharp_data_parquet/*.parquet')\n",
    "print(f\"Found {len(sharp_files)} daily SHARP data files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ff6eb6b-6315-425b-ab21-1795ae28d2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 973 entries, 0 to 972\n",
      "Data columns (total 19 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   T_REC     973 non-null    object \n",
      " 1   HARPNUM   973 non-null    int64  \n",
      " 2   USFLUX    973 non-null    float64\n",
      " 3   MEANGAM   958 non-null    float64\n",
      " 4   MEANGBT   958 non-null    float64\n",
      " 5   MEANGBZ   958 non-null    float64\n",
      " 6   MEANGBH   958 non-null    float64\n",
      " 7   TOTUSJH   973 non-null    float64\n",
      " 8   TOTPOT    973 non-null    float64\n",
      " 9   MEANPOT   958 non-null    float64\n",
      " 10  TOTUSJZ   973 non-null    float64\n",
      " 11  SAVNCPP   973 non-null    float64\n",
      " 12  ABSNJZH   973 non-null    float64\n",
      " 13  AREA_ACR  973 non-null    float64\n",
      " 14  MEANJZH   958 non-null    float64\n",
      " 15  R_VALUE   973 non-null    float64\n",
      " 16  LAT_FWT   973 non-null    float64\n",
      " 17  MEANALP   958 non-null    float64\n",
      " 18  MEANSHR   958 non-null    float64\n",
      "dtypes: float64(17), int64(1), object(1)\n",
      "memory usage: 144.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load the first file found\n",
    "df_sharp_sample = pd.read_parquet(sharp_files[0])\n",
    "# Print a summary of the columns and their data types\n",
    "print(df_sharp_sample.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f518510-d72b-405e-91b9-743448c8a5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 730 daily flare data files.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "flare_files = glob.glob('raw_flare_data_parquet/*.parquet')\n",
    "print(f\"Found {len(flare_files)} daily flare data files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0025700-7c98-42c2-97d5-cff4ea511648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      event_starttime fl_goescls  ar_noaanum\n",
      "0 2014-10-24 02:35:00       C4.2       12192\n",
      "1 2014-10-24 02:55:00       C3.4       12192\n",
      "2 2014-10-24 03:56:00       C3.6       12192\n",
      "3 2014-10-24 07:37:00       M4.0       12192\n",
      "4 2014-10-24 09:58:00       C3.6       12192\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load a flare file (make sure it's one that isn't empty)\n",
    "# You might need to try a few different dates from your folder\n",
    "df_flare_sample = pd.read_parquet('raw_flare_data_parquet/2014-10-24.parquet') \n",
    "# Print the first few rows\n",
    "print(df_flare_sample.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "225a0316-f726-4d6f-a496-ead13548e611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NEW: Drop any rows with missing feature values ---\n",
    "sharp_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bff8265b-6af9-4708-89e8-a676fa48991d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading all raw data chunks ---\n",
      "Loaded 965310 SHARP records.\n",
      "Loaded 4380 flare records.\n",
      "\n",
      "--- Step 2: Cleaning and preparing the complete dataset ---\n",
      "After cleaning, 927593 SHARP records remain.\n",
      "All data cleaned and sorted.\n",
      "\n",
      "--- Step 3: Creating labels based on a 24-hour window ---\n",
      "\n",
      "--- Step 4: Saving final labeled dataset ---\n",
      "\n",
      "--- Dataset Summary ---\n",
      "Total SHARP records: 927593\n",
      "\n",
      "Binary Label Counts ('flare' column):\n",
      "flare\n",
      "0    927593\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Multi-class Label Counts ('classification' column):\n",
      "classification\n",
      "Non-flare    927593\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "✅ Successfully saved final labeled dataset to final_labeled_solar_dataset.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from datetime import timedelta\n",
    "\n",
    "# =========================\n",
    "# CONFIGURATION\n",
    "# =========================\n",
    "SHARP_DIR_INPUT = 'raw_sharp_data_parquet'\n",
    "FLARE_DIR_INPUT = 'raw_flare_data_parquet'\n",
    "FINAL_OUTPUT_FILE = 'final_labeled_solar_dataset.parquet'\n",
    "PREDICTION_WINDOW_HOURS = 24\n",
    "\n",
    "# =========================\n",
    "# 1. LOAD AND COMBINE RAW DATA\n",
    "# =========================\n",
    "print(\"--- Step 1: Loading all raw data chunks ---\")\n",
    "sharp_files = glob.glob(f'{SHARP_DIR_INPUT}/*.parquet')\n",
    "flare_files = glob.glob(f'{FLARE_DIR_INPUT}/*.parquet')\n",
    "\n",
    "if not sharp_files:\n",
    "    raise FileNotFoundError(f\"No SHARP files found in '{SHARP_DIR_INPUT}'.\")\n",
    "\n",
    "sharp_df = pd.concat([pd.read_parquet(f) for f in sharp_files], ignore_index=True)\n",
    "print(f\"Loaded {len(sharp_df)} SHARP records.\")\n",
    "\n",
    "if flare_files:\n",
    "    flares_df = pd.concat([pd.read_parquet(f) for f in flare_files], ignore_index=True)\n",
    "    print(f\"Loaded {len(flares_df)} flare records.\")\n",
    "else:\n",
    "    flares_df = pd.DataFrame()\n",
    "    print(\"No flare data files found.\")\n",
    "\n",
    "# =========================\n",
    "# 2. CLEAN AND PREPARE DATA\n",
    "# =========================\n",
    "print(\"\\n--- Step 2: Cleaning and preparing the complete dataset ---\")\n",
    "\n",
    "# --- THE FIX: Added the format code to correctly parse the T_REC date string ---\n",
    "sharp_df['T_REC'] = pd.to_datetime(sharp_df['T_REC'], format=\"%Y.%m.%d_%H:%M:%S_TAI\", errors='coerce')\n",
    "sharp_df.dropna(subset=['T_REC'], inplace=True) # Drop rows that failed to parse\n",
    "\n",
    "sharp_df.sort_values(by=['HARPNUM', 'T_REC'], inplace=True)\n",
    "sharp_df.drop_duplicates(subset=['HARPNUM', 'T_REC'], inplace=True)\n",
    "sharp_df['HARPNUM'] = sharp_df['HARPNUM'].astype(int)\n",
    "\n",
    "# Drop any rows with missing feature values\n",
    "sharp_df.dropna(inplace=True)\n",
    "print(f\"After cleaning, {len(sharp_df)} SHARP records remain.\")\n",
    "\n",
    "sharp_df['classification'] = 'Non-flare'\n",
    "sharp_df['flare'] = 0\n",
    "\n",
    "if not flares_df.empty:\n",
    "    flares_df['event_starttime'] = pd.to_datetime(flares_df['event_starttime'], errors='coerce')\n",
    "    flares_df.dropna(subset=['event_starttime', 'ar_noaanum', 'fl_goescls'], inplace=True)\n",
    "    flares_df['ar_noaanum'] = pd.to_numeric(flares_df['ar_noaanum'], errors='coerce')\n",
    "    flares_df.dropna(subset=['ar_noaanum'], inplace=True)\n",
    "    flares_df = flares_df[flares_df['ar_noaanum'] > 0].copy()\n",
    "    flares_df['ar_noaanum'] = flares_df['ar_noaanum'].astype(int)\n",
    "    flares_df.sort_values('event_starttime', inplace=True)\n",
    "    print(\"All data cleaned and sorted.\")\n",
    "\n",
    "# =========================\n",
    "# 3. CREATE LABELS\n",
    "# =========================\n",
    "if not flares_df.empty:\n",
    "    print(f\"\\n--- Step 3: Creating labels based on a {PREDICTION_WINDOW_HOURS}-hour window ---\")\n",
    "    prediction_window = pd.Timedelta(hours=PREDICTION_WINDOW_HOURS)\n",
    "    flare_classes = ['X', 'M', 'C', 'B']\n",
    "\n",
    "    for f_class in flare_classes:\n",
    "        class_flares = flares_df[flares_df['fl_goescls'].str.startswith(f_class, na=False)]\n",
    "        for _, flare in class_flares.iterrows():\n",
    "            mask = ((sharp_df['HARPNUM'] == flare['ar_noaanum']) &\n",
    "                    (sharp_df['T_REC'] >= flare['event_starttime'] - prediction_window) &\n",
    "                    (sharp_df['T_REC'] < flare['event_starttime']))\n",
    "            sharp_df.loc[mask, 'classification'] = f_class\n",
    "\n",
    "    sharp_df['flare'] = sharp_df['classification'].isin(['X', 'M']).astype(int)\n",
    "\n",
    "# =========================\n",
    "# 4. SAVE FINAL DATASET\n",
    "# =========================\n",
    "print(\"\\n--- Step 4: Saving final labeled dataset ---\")\n",
    "sharp_df.to_parquet(FINAL_OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"\\n--- Dataset Summary ---\")\n",
    "print(f\"Total SHARP records: {len(sharp_df)}\")\n",
    "print(\"\\nBinary Label Counts ('flare' column):\")\n",
    "print(sharp_df['flare'].value_counts())\n",
    "print(\"\\nMulti-class Label Counts ('classification' column):\")\n",
    "print(sharp_df['classification'].value_counts())\n",
    "print(\"-------------------------\")\n",
    "print(f\"✅ Successfully saved final labeled dataset to {FINAL_OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f259b8ef-15e4-4829-9cdb-c17eb1de3b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Final Labeled Dataset and Raw Flare Data ---\n",
      "\n",
      "--- Finding a powerful flare to verify ---\n",
      "Found a X1.7 flare from AR #11748 at 2013-05-13 01:53:00\n",
      "\n",
      "--- Checking labels in the 24-hour window before the flare ---\n",
      "\n",
      "RESULT: ❌ Verification FAILED. No SHARP records were found in the pre-flare window in the final dataset.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from datetime import timedelta\n",
    "\n",
    "# =========================\n",
    "# CONFIGURATION\n",
    "# =========================\n",
    "FINAL_DATASET_FILE = 'final_labeled_solar_dataset.parquet'\n",
    "FLARE_DIR_INPUT = 'raw_flare_data_parquet'\n",
    "\n",
    "# =========================\n",
    "# 1. LOAD THE DATA\n",
    "# =========================\n",
    "print(\"--- Loading Final Labeled Dataset and Raw Flare Data ---\")\n",
    "try:\n",
    "    # Load the final dataset we want to check\n",
    "    final_df = pd.read_parquet(FINAL_DATASET_FILE)\n",
    "    \n",
    "    # Load the raw flare files to find a known event\n",
    "    flare_files = glob.glob(f'{FLARE_DIR_INPUT}/*.parquet')\n",
    "    flares_df = pd.concat([pd.read_parquet(f) for f in flare_files], ignore_index=True)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Could not find necessary files. {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Clean the flare data to find our target ---\n",
    "flares_df['event_starttime'] = pd.to_datetime(flares_df['event_starttime'], errors='coerce')\n",
    "flares_df.dropna(subset=['event_starttime', 'ar_noaanum', 'fl_goescls'], inplace=True)\n",
    "flares_df['ar_noaanum'] = pd.to_numeric(flares_df['ar_noaanum'], errors='coerce')\n",
    "flares_df = flares_df[flares_df['ar_noaanum'] > 0].copy()\n",
    "flares_df['ar_noaanum'] = flares_df['ar_noaanum'].astype(int)\n",
    "flares_df.sort_values('event_starttime', inplace=True)\n",
    "\n",
    "# =========================\n",
    "# 2. FIND A TARGET FLARE\n",
    "# =========================\n",
    "print(\"\\n--- Finding a powerful flare to verify ---\")\n",
    "# Look for the strongest flare class available (X first, then M)\n",
    "target_flares = flares_df[flares_df['fl_goescls'].str.startswith('X', na=False)]\n",
    "if target_flares.empty:\n",
    "    target_flares = flares_df[flares_df['fl_goescls'].str.startswith('M', na=False)]\n",
    "\n",
    "if target_flares.empty:\n",
    "    print(\"No M-class or X-class flares found in the raw data to verify against.\")\n",
    "else:\n",
    "    # Pick the first strong flare from the list\n",
    "    target_flare = target_flares.iloc[0]\n",
    "    flare_time = target_flare['event_starttime']\n",
    "    ar_num = target_flare['ar_noaanum']\n",
    "    flare_class = target_flare['fl_goescls']\n",
    "    \n",
    "    print(f\"Found a {flare_class} flare from AR #{ar_num} at {flare_time}\")\n",
    "\n",
    "    # =========================\n",
    "    # 3. VERIFY THE LABELS IN THE FINAL DATASET\n",
    "    # =========================\n",
    "    print(\"\\n--- Checking labels in the 24-hour window before the flare ---\")\n",
    "    prediction_window = pd.Timedelta(hours=24)\n",
    "    start_window = flare_time - prediction_window\n",
    "    end_window = flare_time\n",
    "\n",
    "    # Filter the final dataset for the records that SHOULD be labeled\n",
    "    verification_df = final_df[\n",
    "        (final_df['HARPNUM'] == ar_num) &\n",
    "        (final_df['T_REC'] >= start_window) &\n",
    "        (final_df['T_REC'] < end_window)\n",
    "    ]\n",
    "\n",
    "    if verification_df.empty:\n",
    "        print(\"\\nRESULT: ❌ Verification FAILED. No SHARP records were found in the pre-flare window in the final dataset.\")\n",
    "    else:\n",
    "        print(f\"Found {len(verification_df)} records in the pre-flare window. Checking their labels...\")\n",
    "        print(\"First 5 records in window:\")\n",
    "        print(verification_df[['T_REC', 'HARPNUM', 'flare', 'classification']].head())\n",
    "        print(\"\\nLast 5 records in window:\")\n",
    "        print(verification_df[['T_REC', 'HARPNUM', 'flare', 'classification']].tail())\n",
    "\n",
    "        # Check if ALL 'flare' labels in this window are 1\n",
    "        if verification_df['flare'].all() == 1:\n",
    "            print(\"\\nRESULT: ✅ Verification successful! All records in the window are correctly labeled as '1'.\")\n",
    "        else:\n",
    "            print(\"\\nRESULT: ❌ Verification FAILED. Some records in the window have an incorrect '0' label.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0991c842-5e9d-4189-9387-3524b5b372be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Final Labeled Dataset and Raw Flare Data ---\n",
      "\n",
      "Filtering data to only include records from 2014 onwards...\n",
      "Found 488247 SHARP records and 2308 flare records from 2014 onwards.\n",
      "\n",
      "--- Finding a powerful flare to verify ---\n",
      "Found a X1.2 flare from AR #11944 at 2014-01-07 18:04:00\n",
      "\n",
      "--- Checking labels in the 24-hour window before the flare ---\n",
      "\n",
      "RESULT: ❌ Verification FAILED. No SHARP records were found in the pre-flare window in the final dataset.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from datetime import timedelta\n",
    "\n",
    "# =========================\n",
    "# CONFIGURATION\n",
    "# =========================\n",
    "FINAL_DATASET_FILE = 'final_labeled_solar_dataset.parquet'\n",
    "FLARE_DIR_INPUT = 'raw_flare_data_parquet'\n",
    "\n",
    "# =========================\n",
    "# 1. LOAD THE DATA\n",
    "# =========================\n",
    "print(\"--- Loading Final Labeled Dataset and Raw Flare Data ---\")\n",
    "try:\n",
    "    final_df = pd.read_parquet(FINAL_DATASET_FILE)\n",
    "    flare_files = glob.glob(f'{FLARE_DIR_INPUT}/*.parquet')\n",
    "    flares_df = pd.concat([pd.read_parquet(f) for f in flare_files], ignore_index=True)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Could not find necessary files. {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Convert date columns first ---\n",
    "final_df['T_REC'] = pd.to_datetime(final_df['T_REC'])\n",
    "flares_df['event_starttime'] = pd.to_datetime(flares_df['event_starttime'], errors='coerce')\n",
    "\n",
    "# --- THE FIX: Filter both dataframes to only include dates from 2014 onwards ---\n",
    "print(\"\\nFiltering data to only include records from 2014 onwards...\")\n",
    "final_df = final_df[final_df['T_REC'] > '2013-12-31'].copy()\n",
    "flares_df = flares_df[flares_df['event_starttime'] > '2013-12-31'].copy()\n",
    "print(f\"Found {len(final_df)} SHARP records and {len(flares_df)} flare records from 2014 onwards.\")\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# --- Clean the flare data to find our target ---\n",
    "flares_df.dropna(subset=['event_starttime', 'ar_noaanum', 'fl_goescls'], inplace=True)\n",
    "flares_df['ar_noaanum'] = pd.to_numeric(flares_df['ar_noaanum'], errors='coerce')\n",
    "flares_df = flares_df[flares_df['ar_noaanum'] > 0].copy()\n",
    "flares_df['ar_noaanum'] = flares_df['ar_noaanum'].astype(int)\n",
    "flares_df.sort_values('event_starttime', inplace=True)\n",
    "\n",
    "# =========================\n",
    "# 2. FIND A TARGET FLARE\n",
    "# =========================\n",
    "print(\"\\n--- Finding a powerful flare to verify ---\")\n",
    "target_flares = flares_df[flares_df['fl_goescls'].str.startswith('X', na=False)]\n",
    "if target_flares.empty:\n",
    "    target_flares = flares_df[flares_df['fl_goescls'].str.startswith('M', na=False)]\n",
    "\n",
    "if target_flares.empty:\n",
    "    print(\"No M-class or X-class flares found in the 2014+ data to verify against.\")\n",
    "else:\n",
    "    target_flare = target_flares.iloc[0]\n",
    "    flare_time = target_flare['event_starttime']\n",
    "    ar_num = target_flare['ar_noaanum']\n",
    "    flare_class = target_flare['fl_goescls']\n",
    "    \n",
    "    print(f\"Found a {flare_class} flare from AR #{ar_num} at {flare_time}\")\n",
    "\n",
    "    # =========================\n",
    "    # 3. VERIFY THE LABELS IN THE FINAL DATASET\n",
    "    # =========================\n",
    "    print(\"\\n--- Checking labels in the 24-hour window before the flare ---\")\n",
    "    prediction_window = pd.Timedelta(hours=24)\n",
    "    start_window = flare_time - prediction_window\n",
    "    end_window = flare_time\n",
    "\n",
    "    verification_df = final_df[\n",
    "        (final_df['HARPNUM'] == ar_num) &\n",
    "        (final_df['T_REC'] >= start_window) &\n",
    "        (final_df['T_REC'] < end_window)\n",
    "    ]\n",
    "\n",
    "    if verification_df.empty:\n",
    "        print(\"\\nRESULT: ❌ Verification FAILED. No SHARP records were found in the pre-flare window in the final dataset.\")\n",
    "    else:\n",
    "        print(f\"Found {len(verification_df)} records in the pre-flare window. Checking their labels...\")\n",
    "        print(\"First 5 records in window:\")\n",
    "        print(verification_df[['T_REC', 'HARPNUM', 'flare', 'classification']].head())\n",
    "        print(\"\\nLast 5 records in window:\")\n",
    "        print(verification_df[['T_REC', 'HARPNUM', 'flare', 'classification']].tail())\n",
    "\n",
    "        if verification_df['flare'].all() == 1:\n",
    "            print(\"\\nRESULT: ✅ Verification successful! All records in the window are correctly labeled as '1'.\")\n",
    "        else:\n",
    "            print(\"\\nRESULT: ❌ Verification FAILED. Some records in the window have an incorrect '0' label.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf69fbbc-66e8-4e5e-a316-f05c19aad4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     T_REC  HARPNUM  NOAA_AR        USFLUX  MEANGAM  MEANGBT  \\\n",
      "0  2015.01.01_00:00:00_TAI     4973    12246  3.272105e+21   59.454   68.212   \n",
      "1  2015.01.01_00:12:00_TAI     4973    12246  3.347525e+21   58.861   67.663   \n",
      "2  2015.01.01_00:24:00_TAI     4973    12246  3.310049e+21   58.952   67.954   \n",
      "3  2015.01.01_00:36:00_TAI     4973    12246  3.313561e+21   58.703   68.925   \n",
      "4  2015.01.01_00:48:00_TAI     4973    12246  3.316598e+21   58.692   68.839   \n",
      "\n",
      "   MEANGBZ  MEANGBH  TOTUSJH        TOTPOT   MEANPOT       TOTUSJZ  \\\n",
      "0   66.337   47.056  185.674  1.066154e+23  17729.69  2.905874e+12   \n",
      "1   66.407   46.233  191.828  1.074211e+23  17342.77  3.059147e+12   \n",
      "2   66.976   46.539  183.896  1.075039e+23  17624.43  2.960252e+12   \n",
      "3   68.356   46.733  188.307  1.060242e+23  17686.05  2.902295e+12   \n",
      "4   67.741   46.193  187.325  1.049565e+23  17350.36  3.035269e+12   \n",
      "\n",
      "        SAVNCPP  ABSNJZH    AREA_ACR   MEANJZH  R_VALUE    LAT_FWT   MEANALP  \\\n",
      "0  7.099509e+11   44.743  109.263023 -0.009881    3.596  18.021978 -0.014575   \n",
      "1  6.942817e+11   46.183  106.783920 -0.009902    3.560  18.022076 -0.014835   \n",
      "2  6.701272e+11   43.820  106.896584 -0.009541    3.558  18.020742 -0.014254   \n",
      "3  8.833789e+11   46.204  106.923767 -0.010236    3.556  18.022167 -0.014970   \n",
      "4  9.456513e+11   43.691  106.882858 -0.009592    3.556  18.017321 -0.014126   \n",
      "\n",
      "   MEANSHR  \n",
      "0   51.532  \n",
      "1   50.939  \n",
      "2   50.829  \n",
      "3   50.537  \n",
      "4   50.768  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"raw_sharp_data_parquet/2015-01-01.parquet\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20735a77-0560-4b4e-8aa7-d210254280f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     T_REC  HARPNUM        USFLUX  MEANGAM  MEANGBT  MEANGBZ  \\\n",
      "0  2014.01.01_00:00:00_TAI     3520  2.425337e+22   30.447   59.207   63.758   \n",
      "1  2014.01.01_00:12:00_TAI     3520  2.397443e+22   29.694   58.999   62.370   \n",
      "2  2014.01.01_00:24:00_TAI     3520  2.368803e+22   29.602   59.602   64.187   \n",
      "3  2014.01.01_00:36:00_TAI     3520  2.362252e+22   30.041   58.261   61.272   \n",
      "4  2014.01.01_00:48:00_TAI     3520  2.307921e+22   29.986   59.347   64.168   \n",
      "\n",
      "   MEANGBH  TOTUSJH        TOTPOT   MEANPOT       TOTUSJZ       SAVNCPP  \\\n",
      "0   28.449  782.811  1.239458e+23  2609.887  1.545143e+13  4.359763e+12   \n",
      "1   28.882  769.198  1.144742e+23  2455.562  1.488627e+13  5.104859e+12   \n",
      "2   28.698  766.378  1.136556e+23  2467.099  1.510670e+13  4.399965e+12   \n",
      "3   27.703  744.015  1.123239e+23  2421.855  1.438206e+13  4.098449e+12   \n",
      "4   27.976  736.776  1.122267e+23  2467.163  1.450569e+13  4.130472e+12   \n",
      "\n",
      "   ABSNJZH    AREA_ACR   MEANJZH  R_VALUE    LAT_FWT   MEANALP  MEANSHR  \n",
      "0   90.057  617.493835  0.002518    4.602 -17.523125  0.007377   22.953  \n",
      "1   93.595  621.271118  0.002666    4.559 -17.495068  0.007765   21.956  \n",
      "2   84.942  610.711060  0.002449    4.434 -17.529327  0.007131   22.091  \n",
      "3   79.779  614.134460  0.002285    4.412 -17.485861  0.006756   22.224  \n",
      "4   87.394  598.042419  0.002552    4.557 -17.518372  0.007589   22.233  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"raw_sharp_data_parquet/2014-01-01.parquet\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93b2ba6f-aa81-46d9-9616-4e40d97f8f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying JSOC for SHARP data for AR #11748 between 2013-05-12T00:00:00 and 2013-05-14T23:59:59...\n",
      "\n",
      "--- QUERY COMPLETE ---\n",
      "Result: Found 213 records.\n",
      "✅ Data was found.\n"
     ]
    }
   ],
   "source": [
    "from sunpy.net import attrs as a\n",
    "from sunpy.net.jsoc import JSOCClient\n",
    "\n",
    "# The Active Region that had a huge X-class flare\n",
    "AR_NUMBER = 11748\n",
    "\n",
    "# The time range around the flare\n",
    "START_TIME = \"2013-05-12T00:00:00\"\n",
    "END_TIME = \"2013-05-14T23:59:59\"\n",
    "\n",
    "print(f\"Querying JSOC for SHARP data for AR #{AR_NUMBER} between {START_TIME} and {END_TIME}...\")\n",
    "\n",
    "try:\n",
    "    jsoc_client = JSOCClient()\n",
    "    \n",
    "    # This is a direct query for the specific data we need\n",
    "    response = jsoc_client.search(\n",
    "        a.Time(START_TIME, END_TIME),\n",
    "        a.jsoc.Series('hmi.sharp_720s'),\n",
    "        a.jsoc.Keyword('NOAA_AR') == AR_NUMBER\n",
    "    )\n",
    "    \n",
    "    print(\"\\n--- QUERY COMPLETE ---\")\n",
    "    print(f\"Result: Found {len(response)} records.\")\n",
    "    \n",
    "    if len(response) > 0:\n",
    "        print(\"✅ Data was found.\")\n",
    "    else:\n",
    "        print(\"\\n❌ This is the definitive proof: The final, science-quality SHARP data for this major, flaring active region is MISSING from the archive.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d655d0a-f7ec-400b-b926-73130500343f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     T_REC  HARPNUM  NOAA_AR        USFLUX  MEANGAM  MEANGBT  \\\n",
      "0  2013.01.01_00:00:00_TAI     2322    11636  4.509503e+21   35.303  141.572   \n",
      "1  2013.01.01_00:12:00_TAI     2322    11636  4.483117e+21   34.914  141.825   \n",
      "2  2013.01.01_00:24:00_TAI     2322    11636  4.500127e+21   34.246  142.994   \n",
      "3  2013.01.01_00:36:00_TAI     2322    11636  4.474255e+21   33.887  142.560   \n",
      "4  2013.01.01_00:48:00_TAI     2322    11636  4.420313e+21   33.728  144.255   \n",
      "\n",
      "   MEANGBZ  MEANGBH  TOTUSJH        TOTPOT   MEANPOT       TOTUSJZ  \\\n",
      "0  141.166   68.409  377.037  2.285518e+22  2343.044  8.023545e+12   \n",
      "1  141.105   68.116  378.145  2.243908e+22  2311.717  8.098751e+12   \n",
      "2  142.356   67.673  378.075  2.221918e+22  2302.926  8.000354e+12   \n",
      "3  142.544   67.399  377.511  2.183968e+22  2300.644  7.891649e+12   \n",
      "4  144.067   68.401  377.976  2.157904e+22  2312.989  7.930061e+12   \n",
      "\n",
      "        SAVNCPP  ABSNJZH    AREA_ACR   MEANJZH  R_VALUE    LAT_FWT   MEANALP  \\\n",
      "0  1.437391e+12   17.238  499.353577 -0.002347    2.736  12.897586 -0.008345   \n",
      "1  7.453824e+11    5.309  501.507446 -0.000726    2.108  12.894959 -0.002595   \n",
      "2  6.336625e+11    6.308  501.338348 -0.000868    2.397  12.922968 -0.003096   \n",
      "3  6.963714e+11    5.630  500.480621 -0.000788    2.927  12.935432 -0.002770   \n",
      "4  7.112941e+11    9.127  502.902924 -0.001299    2.692  12.910390 -0.004535   \n",
      "\n",
      "   MEANSHR  \n",
      "0   25.838  \n",
      "1   25.676  \n",
      "2   25.059  \n",
      "3   24.847  \n",
      "4   24.686  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"raw_sharp_data_parquet_new/2013-01-01.parquet\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7d8f723f-a79c-4284-a1f2-4c8700264a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading all raw data chunks ---\n",
      "Loaded 962956 SHARP records.\n",
      "Loaded 548 flare records.\n",
      "\n",
      "--- Step 2: Cleaning and preparing the complete dataset ---\n",
      "\n",
      "--- Step 4: Saving final labeled dataset ---\n",
      "\n",
      "--- Dataset Summary ---\n",
      "Total SHARP records: 631316\n",
      "\n",
      "Binary Label Counts ('flare' column):\n",
      "flare\n",
      "0    631316\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Multi-class Label Counts ('classification' column):\n",
      "classification\n",
      "Non-flare    631316\n",
      "Name: count, dtype: int64\n",
      "✅ Successfully saved to final_labeled_solar_dataset.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from datetime import timedelta\n",
    "\n",
    "# =========================\n",
    "# CONFIGURATION\n",
    "# =========================\n",
    "SHARP_DIR_INPUT = 'raw_sharp_data_parquet_new'\n",
    "FLARE_DIR_INPUT = 'raw_flare_data_parquet_new'\n",
    "FINAL_OUTPUT_FILE = 'final_labeled_solar_dataset.parquet'\n",
    "PREDICTION_WINDOW_HOURS = 24\n",
    "\n",
    "# =========================\n",
    "# 1. LOAD AND COMBINE RAW DATA\n",
    "# =========================\n",
    "print(\"--- Step 1: Loading all raw data chunks ---\")\n",
    "sharp_files = glob.glob(f'{SHARP_DIR_INPUT}/*.parquet')\n",
    "flare_files = glob.glob(f'{FLARE_DIR_INPUT}/*.parquet')\n",
    "\n",
    "if not sharp_files:\n",
    "    raise FileNotFoundError(f\"No SHARP files found in '{SHARP_DIR_INPUT}'.\")\n",
    "\n",
    "sharp_df = pd.concat([pd.read_parquet(f) for f in sharp_files if os.path.getsize(f) > 0], ignore_index=True)\n",
    "print(f\"Loaded {len(sharp_df)} SHARP records.\")\n",
    "\n",
    "if flare_files:\n",
    "    flares_df = pd.concat([pd.read_parquet(f) for f in flare_files if os.path.getsize(f) > 0], ignore_index=True)\n",
    "    print(f\"Loaded {len(flares_df)} flare records.\")\n",
    "else:\n",
    "    flares_df = pd.DataFrame()\n",
    "\n",
    "# =========================\n",
    "# 2. CLEAN AND PREPARE DATA\n",
    "# =========================\n",
    "print(\"\\n--- Step 2: Cleaning and preparing the complete dataset ---\")\n",
    "sharp_df['T_REC'] = pd.to_datetime(sharp_df['T_REC'], format=\"%Y.%m.%d_%H:%M:%S_TAI\", errors='coerce')\n",
    "sharp_df.dropna(subset=['T_REC'], inplace=True)\n",
    "sharp_df.sort_values(by=['HARPNUM', 'T_REC'], inplace=True)\n",
    "sharp_df.drop_duplicates(subset=['HARPNUM', 'T_REC'], inplace=True)\n",
    "sharp_df.dropna(inplace=True) # Drop rows with any missing feature values\n",
    "sharp_df['HARPNUM'] = sharp_df['HARPNUM'].astype(int)\n",
    "sharp_df['NOAA_AR'] = sharp_df['NOAA_AR'].astype(int)\n",
    "\n",
    "sharp_df['classification'] = 'Non-flare'\n",
    "sharp_df['flare'] = 0\n",
    "\n",
    "if not flares_df.empty:\n",
    "    flares_df['event_starttime'] = pd.to_datetime(flares_df['event_starttime'], errors='coerce')\n",
    "    flares_df.dropna(subset=['event_starttime', 'ar_noaanum', 'fl_goescls'], inplace=True)\n",
    "    flares_df['ar_noaanum'] = pd.to_numeric(flares_df['ar_noaanum'], errors='coerce')\n",
    "    flares_df = flares_df[flares_df['ar_noaanum'] > 0].copy()\n",
    "    flares_df['ar_noaanum'] = flares_df['ar_noaanum'].astype(int)\n",
    "\n",
    "# =========================\n",
    "# 3. CREATE LABELS\n",
    "# =========================\n",
    "if not flares_df.empty:\n",
    "    print(f\"\\n--- Step 3: Creating labels...\")\n",
    "    prediction_window = pd.Timedelta(hours=PREDICTION_WINDOW_HOURS)\n",
    "    flare_classes = ['X', 'M', 'C', 'B']\n",
    "\n",
    "    for f_class in flare_classes:\n",
    "        class_flares = flares_df[flares_df['fl_goescls'].str.startswith(f_class, na=False)]\n",
    "        for _, flare in class_flares.iterrows():\n",
    "            mask = ((sharp_df['NOAA_AR'] == flare['ar_noaanum']) &\n",
    "                    (sharp_df['T_REC'] >= flare['event_starttime'] - prediction_window) &\n",
    "                    (sharp_df['T_REC'] < flare['event_starttime']))\n",
    "            sharp_df.loc[mask, 'classification'] = f_class\n",
    "\n",
    "    sharp_df['flare'] = sharp_df['classification'].isin(['X', 'M']).astype(int)\n",
    "\n",
    "# =========================\n",
    "# 4. SAVE FINAL DATASET\n",
    "# =========================\n",
    "print(\"\\n--- Step 4: Saving final labeled dataset ---\")\n",
    "sharp_df.to_parquet(FINAL_OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"\\n--- Dataset Summary ---\")\n",
    "print(f\"Total SHARP records: {len(sharp_df)}\")\n",
    "print(\"\\nBinary Label Counts ('flare' column):\")\n",
    "print(sharp_df['flare'].value_counts())\n",
    "print(\"\\nMulti-class Label Counts ('classification' column):\")\n",
    "print(sharp_df['classification'].value_counts())\n",
    "print(f\"✅ Successfully saved to {FINAL_OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5bfe2297-25c8-4f5a-a108-9ebbf53323c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading all raw data chunks ---\n",
      "Loaded 962956 SHARP records.\n",
      "Loaded 4380 flare records.\n",
      "\n",
      "--- Step 2: Cleaning and Verifying Data Alignment ---\n",
      "\n",
      "--- Sanity Check ---\n",
      "Found 426 unique, valid AR numbers in the SHARP data.\n",
      "Found 372 unique, valid AR numbers in the Flare data.\n",
      "\n",
      ">> SUCCESS: Found 261 matching AR numbers. Proceeding with labeling.\n",
      "--------------------\n",
      "\n",
      "--- Step 3: Creating labels...\n",
      "\n",
      "--- Step 4: Saving final labeled dataset ---\n",
      "\n",
      "--- Dataset Summary ---\n",
      "Total SHARP records: 346372\n",
      "\n",
      "Binary Label Counts ('flare' column):\n",
      "flare\n",
      "0    344158\n",
      "1      2214\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Multi-class Label Counts ('classification' column):\n",
      "classification\n",
      "Non-flare    258691\n",
      "C             61663\n",
      "B             23804\n",
      "M              2153\n",
      "X                61\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "✅ Successfully saved to final_labeled_solar_dataset.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from datetime import timedelta\n",
    "\n",
    "# =========================\n",
    "# CONFIGURATION\n",
    "# =========================\n",
    "SHARP_DIR_INPUT = 'raw_sharp_data_parquet_new'\n",
    "FLARE_DIR_INPUT = 'raw_flare_data_parquet'\n",
    "FINAL_OUTPUT_FILE = 'final_labeled_solar_dataset.parquet'\n",
    "PREDICTION_WINDOW_HOURS = 24\n",
    "\n",
    "# =========================\n",
    "# 1. LOAD AND COMBINE RAW DATA\n",
    "# =========================\n",
    "print(\"--- Step 1: Loading all raw data chunks ---\")\n",
    "sharp_files = glob.glob(f'{SHARP_DIR_INPUT}/*.parquet')\n",
    "flare_files = glob.glob(f'{FLARE_DIR_INPUT}/*.parquet')\n",
    "\n",
    "if not sharp_files:\n",
    "    raise FileNotFoundError(f\"No SHARP files found in '{SHARP_DIR_INPUT}'.\")\n",
    "\n",
    "sharp_df = pd.concat([pd.read_parquet(f) for f in sharp_files if os.path.getsize(f) > 0], ignore_index=True)\n",
    "print(f\"Loaded {len(sharp_df)} SHARP records.\")\n",
    "\n",
    "if flare_files:\n",
    "    flares_df = pd.concat([pd.read_parquet(f) for f in flare_files if os.path.getsize(f) > 0], ignore_index=True)\n",
    "    print(f\"Loaded {len(flares_df)} flare records.\")\n",
    "else:\n",
    "    flares_df = pd.DataFrame()\n",
    "\n",
    "# =========================\n",
    "# 2. ROBUST CLEANING AND VERIFICATION\n",
    "# =========================\n",
    "print(\"\\n--- Step 2: Cleaning and Verifying Data Alignment ---\")\n",
    "\n",
    "# --- Clean SHARP data ---\n",
    "sharp_df['T_REC'] = pd.to_datetime(sharp_df['T_REC'], format=\"%Y.%m.%d_%H:%M:%S_TAI\", errors='coerce')\n",
    "sharp_df.dropna(subset=['T_REC'], inplace=True)\n",
    "sharp_df.sort_values(by=['HARPNUM', 'T_REC'], inplace=True)\n",
    "sharp_df.drop_duplicates(subset=['HARPNUM', 'T_REC'], inplace=True)\n",
    "sharp_df.dropna(inplace=True) # Drop rows with any missing feature values\n",
    "\n",
    "# Robustly clean the NOAA_AR column\n",
    "if 'NOAA_AR' in sharp_df.columns:\n",
    "    sharp_df['NOAA_AR'] = pd.to_numeric(sharp_df['NOAA_AR'], errors='coerce')\n",
    "    sharp_df.dropna(subset=['NOAA_AR'], inplace=True)\n",
    "    sharp_df = sharp_df[sharp_df['NOAA_AR'] > 0].copy()\n",
    "    sharp_df['NOAA_AR'] = sharp_df['NOAA_AR'].astype(int)\n",
    "    sharp_ar_numbers = set(sharp_df['NOAA_AR'].unique())\n",
    "else:\n",
    "    raise KeyError(\"The 'NOAA_AR' column is missing from the raw SHARP data.\")\n",
    "\n",
    "# --- Clean Flare data ---\n",
    "if not flares_df.empty:\n",
    "    flares_df['event_starttime'] = pd.to_datetime(flares_df['event_starttime'], errors='coerce')\n",
    "    flares_df.dropna(subset=['event_starttime', 'ar_noaanum', 'fl_goescls'], inplace=True)\n",
    "    flares_df['ar_noaanum'] = pd.to_numeric(flares_df['ar_noaanum'], errors='coerce')\n",
    "    flares_df.dropna(subset=['ar_noaanum'], inplace=True)\n",
    "    flares_df = flares_df[flares_df['ar_noaanum'] > 0].copy()\n",
    "    flares_df['ar_noaanum'] = flares_df['ar_noaanum'].astype(int)\n",
    "    flare_ar_numbers = set(flares_df['ar_noaanum'].unique())\n",
    "else:\n",
    "    flare_ar_numbers = set()\n",
    "\n",
    "# --- Run the Sanity Check ---\n",
    "print(\"\\n--- Sanity Check ---\")\n",
    "print(f\"Found {len(sharp_ar_numbers)} unique, valid AR numbers in the SHARP data.\")\n",
    "print(f\"Found {len(flare_ar_numbers)} unique, valid AR numbers in the Flare data.\")\n",
    "matching_ar_numbers = sharp_ar_numbers.intersection(flare_ar_numbers)\n",
    "if not matching_ar_numbers:\n",
    "    print(\"\\n>> CRITICAL: No matching AR numbers found. Cannot proceed with labeling.\")\n",
    "    exit()\n",
    "else:\n",
    "    print(f\"\\n>> SUCCESS: Found {len(matching_ar_numbers)} matching AR numbers. Proceeding with labeling.\")\n",
    "print(\"--------------------\")\n",
    "\n",
    "# =========================\n",
    "# 3. CREATE LABELS\n",
    "# =========================\n",
    "sharp_df['classification'] = 'Non-flare'\n",
    "sharp_df['flare'] = 0\n",
    "\n",
    "if not flares_df.empty:\n",
    "    print(f\"\\n--- Step 3: Creating labels...\")\n",
    "    prediction_window = pd.Timedelta(hours=PREDICTION_WINDOW_HOURS)\n",
    "    flare_classes = ['X', 'M', 'C', 'B']\n",
    "\n",
    "    for f_class in flare_classes:\n",
    "        class_flares = flares_df[flares_df['fl_goescls'].str.startswith(f_class, na=False)]\n",
    "        for _, flare in class_flares.iterrows():\n",
    "            mask = ((sharp_df['NOAA_AR'] == flare['ar_noaanum']) &\n",
    "                    (sharp_df['T_REC'] >= flare['event_starttime'] - prediction_window) &\n",
    "                    (sharp_df['T_REC'] < flare['event_starttime']))\n",
    "            sharp_df.loc[mask, 'classification'] = f_class\n",
    "\n",
    "    sharp_df['flare'] = sharp_df['classification'].isin(['X', 'M']).astype(int)\n",
    "\n",
    "# =========================\n",
    "# 4. SAVE FINAL DATASET\n",
    "# =========================\n",
    "print(\"\\n--- Step 4: Saving final labeled dataset ---\")\n",
    "sharp_df.to_parquet(FINAL_OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"\\n--- Dataset Summary ---\")\n",
    "print(f\"Total SHARP records: {len(sharp_df)}\")\n",
    "print(\"\\nBinary Label Counts ('flare' column):\")\n",
    "print(sharp_df['flare'].value_counts())\n",
    "print(\"\\nMulti-class Label Counts ('classification' column):\")\n",
    "print(sharp_df['classification'].value_counts())\n",
    "print(\"-------------------------\")\n",
    "print(f\"✅ Successfully saved to {FINAL_OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ca68c356-7270-46ca-8797-a9b19128e7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading all raw data chunks ---\n",
      "Loaded 962956 SHARP records.\n",
      "Loaded 4380 flare records.\n",
      "\n",
      "--- Step 2: Cleaning and Converting Time ---\n",
      "Converting SHARP timestamps from TAI → UTC ...\n",
      "Before conversion: NaT to NaT\n",
      "After conversion : nan to nan\n",
      "\n",
      "--- Sanity Check ---\n",
      "Found 0 unique AR numbers in SHARP.\n",
      "Found 372 unique AR numbers in Flares.\n",
      ">> CRITICAL: No matching AR numbers found. Exiting.\n",
      "--------------------\n",
      "\n",
      "--- Step 3: Creating Labels (window=24h) ---\n",
      "Labeled 0 rows with flare classes.\n",
      "\n",
      "--- Step 4: Saving Final Dataset ---\n",
      "\n",
      "--- Dataset Summary ---\n",
      "Total SHARP records: 0\n",
      "Binary label counts:\n",
      " Series([], Name: count, dtype: int64)\n",
      "Multi-class counts:\n",
      " Series([], Name: count, dtype: int64)\n",
      "-------------------------\n",
      "✅ Saved to final_labeled_solar_dataset.parquet\n",
      "\n",
      "⚠️ No flare matches found in preview. Check AR number alignment!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from astropy.time import Time\n",
    "\n",
    "# =========================\n",
    "# CONFIGURATION\n",
    "# =========================\n",
    "SHARP_DIR_INPUT = 'raw_sharp_data_parquet_new'\n",
    "FLARE_DIR_INPUT = 'raw_flare_data_parquet'\n",
    "FINAL_OUTPUT_FILE = 'final_labeled_solar_dataset.parquet'\n",
    "PREDICTION_WINDOW_HOURS = 24\n",
    "\n",
    "# =========================\n",
    "# 1. LOAD AND COMBINE RAW DATA\n",
    "# =========================\n",
    "print(\"--- Step 1: Loading all raw data chunks ---\")\n",
    "sharp_files = glob.glob(f'{SHARP_DIR_INPUT}/*.parquet')\n",
    "flare_files = glob.glob(f'{FLARE_DIR_INPUT}/*.parquet')\n",
    "\n",
    "if not sharp_files:\n",
    "    raise FileNotFoundError(f\"No SHARP files found in '{SHARP_DIR_INPUT}'.\")\n",
    "\n",
    "sharp_df = pd.concat([pd.read_parquet(f) for f in sharp_files if os.path.getsize(f) > 0], ignore_index=True)\n",
    "print(f\"Loaded {len(sharp_df)} SHARP records.\")\n",
    "\n",
    "if flare_files:\n",
    "    flares_df = pd.concat([pd.read_parquet(f) for f in flare_files if os.path.getsize(f) > 0], ignore_index=True)\n",
    "    print(f\"Loaded {len(flares_df)} flare records.\")\n",
    "else:\n",
    "    flares_df = pd.DataFrame()\n",
    "\n",
    "# =========================\n",
    "# 2. CLEAN & TIMESTAMP CONVERSION\n",
    "# =========================\n",
    "print(\"\\n--- Step 2: Cleaning and Converting Time ---\")\n",
    "\n",
    "# --- Clean SHARP ---\n",
    "sharp_df['T_REC'] = pd.to_datetime(sharp_df['T_REC'], errors='coerce')\n",
    "sharp_df.dropna(subset=['T_REC'], inplace=True)\n",
    "sharp_df.sort_values(by=['HARPNUM', 'T_REC'], inplace=True)\n",
    "sharp_df.drop_duplicates(subset=['HARPNUM', 'T_REC'], inplace=True)\n",
    "\n",
    "print(\"Converting SHARP timestamps from TAI → UTC ...\")\n",
    "print(\"Before conversion:\", sharp_df['T_REC'].min(), \"to\", sharp_df['T_REC'].max())\n",
    "\n",
    "sharp_times = Time(list(sharp_df['T_REC']), format=\"datetime\", scale=\"tai\")\n",
    "sharp_df['T_REC'] = sharp_times.utc.datetime\n",
    "\n",
    "print(\"After conversion :\", sharp_df['T_REC'].min(), \"to\", sharp_df['T_REC'].max())\n",
    "\n",
    "# --- Clean NOAA_AR ---\n",
    "if 'NOAA_AR' not in sharp_df.columns:\n",
    "    raise KeyError(\"The 'NOAA_AR' column is missing from SHARP data.\")\n",
    "\n",
    "sharp_df['NOAA_AR'] = pd.to_numeric(sharp_df['NOAA_AR'], errors='coerce')\n",
    "sharp_df.dropna(subset=['NOAA_AR'], inplace=True)\n",
    "sharp_df = sharp_df[sharp_df['NOAA_AR'] > 0].copy()\n",
    "sharp_df['NOAA_AR'] = sharp_df['NOAA_AR'].astype(int)\n",
    "sharp_ar_numbers = set(sharp_df['NOAA_AR'].unique())\n",
    "\n",
    "# --- Clean Flare data ---\n",
    "if not flares_df.empty:\n",
    "    flares_df['event_starttime'] = pd.to_datetime(flares_df['event_starttime'], errors='coerce')\n",
    "    flares_df.dropna(subset=['event_starttime', 'ar_noaanum', 'fl_goescls'], inplace=True)\n",
    "    flares_df['ar_noaanum'] = pd.to_numeric(flares_df['ar_noaanum'], errors='coerce')\n",
    "    flares_df.dropna(subset=['ar_noaanum'], inplace=True)\n",
    "    flares_df = flares_df[flares_df['ar_noaanum'] > 0].copy()\n",
    "    flares_df['ar_noaanum'] = flares_df['ar_noaanum'].astype(int)\n",
    "    flare_ar_numbers = set(flares_df['ar_noaanum'].unique())\n",
    "else:\n",
    "    flare_ar_numbers = set()\n",
    "\n",
    "# --- Sanity Check ---\n",
    "print(\"\\n--- Sanity Check ---\")\n",
    "print(f\"Found {len(sharp_ar_numbers)} unique AR numbers in SHARP.\")\n",
    "print(f\"Found {len(flare_ar_numbers)} unique AR numbers in Flares.\")\n",
    "matching_ar_numbers = sharp_ar_numbers.intersection(flare_ar_numbers)\n",
    "if not matching_ar_numbers:\n",
    "    print(\">> CRITICAL: No matching AR numbers found. Exiting.\")\n",
    "    exit()\n",
    "else:\n",
    "    print(f\">> SUCCESS: Found {len(matching_ar_numbers)} matching AR numbers.\")\n",
    "print(\"--------------------\")\n",
    "\n",
    "# =========================\n",
    "# 3. CREATE LABELS\n",
    "# =========================\n",
    "sharp_df['classification'] = 'Non-flare'\n",
    "sharp_df['flare'] = 0\n",
    "\n",
    "if not flares_df.empty:\n",
    "    print(f\"\\n--- Step 3: Creating Labels (window={PREDICTION_WINDOW_HOURS}h) ---\")\n",
    "    prediction_window = pd.Timedelta(hours=PREDICTION_WINDOW_HOURS)\n",
    "    flare_classes = ['X', 'M', 'C', 'B']\n",
    "\n",
    "    labeled_rows = 0\n",
    "    for f_class in flare_classes:\n",
    "        class_flares = flares_df[flares_df['fl_goescls'].str.startswith(f_class, na=False)]\n",
    "        for _, flare in class_flares.iterrows():\n",
    "            mask = ((sharp_df['NOAA_AR'] == flare['ar_noaanum']) &\n",
    "                    (sharp_df['T_REC'] >= flare['event_starttime'] - prediction_window) &\n",
    "                    (sharp_df['T_REC'] < flare['event_starttime']))\n",
    "            sharp_df.loc[mask, 'classification'] = f_class\n",
    "            labeled_rows += mask.sum()\n",
    "\n",
    "    sharp_df['flare'] = sharp_df['classification'].isin(['X', 'M']).astype(int)\n",
    "    print(f\"Labeled {labeled_rows} rows with flare classes.\")\n",
    "\n",
    "# =========================\n",
    "# 4. SAVE + DEBUG PREVIEW\n",
    "# =========================\n",
    "print(\"\\n--- Step 4: Saving Final Dataset ---\")\n",
    "sharp_df.to_parquet(FINAL_OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"\\n--- Dataset Summary ---\")\n",
    "print(f\"Total SHARP records: {len(sharp_df)}\")\n",
    "print(\"Binary label counts:\\n\", sharp_df['flare'].value_counts())\n",
    "print(\"Multi-class counts:\\n\", sharp_df['classification'].value_counts())\n",
    "print(\"-------------------------\")\n",
    "print(f\"✅ Saved to {FINAL_OUTPUT_FILE}\")\n",
    "\n",
    "# --- Debug Preview: Show first matches ---\n",
    "debug_matches = sharp_df[sharp_df['classification'] != 'Non-flare'].head(20)\n",
    "if not debug_matches.empty:\n",
    "    print(\"\\n--- Sample Flare Matches ---\")\n",
    "    print(debug_matches[['T_REC', 'NOAA_AR', 'classification', 'flare']])\n",
    "else:\n",
    "    print(\"\\n⚠️ No flare matches found in preview. Check AR number alignment!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "690001c7-6348-46db-a17b-a231e33f23ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and cleaning data ---\n",
      "Filtered to 2055 usable flare records from 2014 onwards.\n",
      "\n",
      "--- Finding a powerful flare to test from 2014 ---\n",
      "\n",
      "Found target flare from AR #11944 at (UTC): 2014-01-07 18:04:00\n",
      "Found 1601 SHARP records for this AR.\n",
      "Prediction window starts at (UTC): 2014-01-06 18:04:00\n",
      "Prediction window ends at (UTC):   2014-01-07 18:04:00\n",
      "\n",
      "--- Comparing SHARP T_REC to the UTC window ---\n",
      "✅ SUCCESS: Found 182 records that fall inside the time window.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from datetime import timedelta\n",
    "\n",
    "# =========================\n",
    "# CONFIGURATION\n",
    "# =========================\n",
    "SHARP_DIR_INPUT = 'raw_sharp_data_parquet_new'\n",
    "FLARE_DIR_INPUT = 'raw_flare_data_parquet'\n",
    "\n",
    "# =========================\n",
    "# 1. LOAD AND CLEAN DATA\n",
    "# =========================\n",
    "print(\"--- Loading and cleaning data ---\")\n",
    "sharp_df = pd.concat([pd.read_parquet(f) for f in glob.glob(f'{SHARP_DIR_INPUT}/*.parquet') if os.path.getsize(f) > 0], ignore_index=True)\n",
    "flares_df = pd.concat([pd.read_parquet(f) for f in glob.glob(f'{FLARE_DIR_INPUT}/*.parquet') if os.path.getsize(f) > 0], ignore_index=True)\n",
    "\n",
    "# Clean SHARP data\n",
    "sharp_df['T_REC_str'] = sharp_df['T_REC'] # Keep original string for inspection\n",
    "sharp_df['T_REC'] = pd.to_datetime(sharp_df['T_REC'], format=\"%Y.%m.%d_%H:%M:%S_TAI\", errors='coerce')\n",
    "sharp_df.dropna(subset=['T_REC', 'NOAA_AR'], inplace=True)\n",
    "sharp_df['NOAA_AR'] = sharp_df['NOAA_AR'].astype(int)\n",
    "\n",
    "# Clean Flare data\n",
    "flares_df['event_starttime'] = pd.to_datetime(flares_df['event_starttime'], errors='coerce')\n",
    "flares_df.dropna(subset=['event_starttime', 'ar_noaanum', 'fl_goescls'], inplace=True)\n",
    "flares_df = flares_df[flares_df['ar_noaanum'] > 0].copy()\n",
    "flares_df['ar_noaanum'] = flares_df['ar_noaanum'].astype(int)\n",
    "flares_df.sort_values('event_starttime', inplace=True)\n",
    "\n",
    "# --- THE FIX: Filter to only include dates from 2014 onwards ---\n",
    "flares_df = flares_df[flares_df['event_starttime'].dt.year >= 2014].copy()\n",
    "print(f\"Filtered to {len(flares_df)} usable flare records from 2014 onwards.\")\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# =========================\n",
    "# 2. FIND A TARGET FLARE AND MATCHING SHARP DATA\n",
    "# =========================\n",
    "print(\"\\n--- Finding a powerful flare to test from 2014 ---\")\n",
    "target_flares = flares_df[flares_df['fl_goescls'].str.startswith('X')]\n",
    "if target_flares.empty:\n",
    "    target_flares = flares_df[flares_df['fl_goescls'].str.startswith('M')]\n",
    "\n",
    "if not target_flares.empty:\n",
    "    target_flare = target_flares.iloc[0]\n",
    "    flare_time = target_flare['event_starttime']\n",
    "    ar_num = target_flare['ar_noaanum']\n",
    "    \n",
    "    print(f\"\\nFound target flare from AR #{ar_num} at (UTC): {flare_time}\")\n",
    "    \n",
    "    # Find all SHARP records for this AR\n",
    "    sharp_subset = sharp_df[sharp_df['NOAA_AR'] == ar_num].copy()\n",
    "    \n",
    "    if not sharp_subset.empty:\n",
    "        print(f\"Found {len(sharp_subset)} SHARP records for this AR.\")\n",
    "        \n",
    "        # Define the 24-hour window\n",
    "        prediction_window = pd.Timedelta(hours=24)\n",
    "        start_window = flare_time - prediction_window\n",
    "        end_window = flare_time\n",
    "        \n",
    "        print(f\"Prediction window starts at (UTC): {start_window}\")\n",
    "        print(f\"Prediction window ends at (UTC):   {end_window}\")\n",
    "        \n",
    "        # Check the timestamps of the SHARP data\n",
    "        print(\"\\n--- Comparing SHARP T_REC to the UTC window ---\")\n",
    "        \n",
    "        sharp_subset['in_window'] = (\n",
    "            (sharp_subset['T_REC'] >= start_window) &\n",
    "            (sharp_subset['T_REC'] < end_window)\n",
    "        )\n",
    "        \n",
    "        num_matches = sharp_subset['in_window'].sum()\n",
    "        \n",
    "        if num_matches > 0:\n",
    "            print(f\"✅ SUCCESS: Found {num_matches} records that fall inside the time window.\")\n",
    "        else:\n",
    "            print(\"❌ FAILURE: Found 0 records inside the time window.\")\n",
    "            print(\"\\nFirst SHARP record for this AR:\")\n",
    "            print(sharp_subset.head(1)[['T_REC_str', 'T_REC']])\n",
    "            print(\"\\nLast SHARP record for this AR:\")\n",
    "            print(sharp_subset.tail(1)[['T_REC_str', 'T_REC']])\n",
    "    else:\n",
    "        print(f\"Could not find any SHARP data for AR #{ar_num}.\")\n",
    "else:\n",
    "    print(\"Could not find any M or X class flares to test in 2014+.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c483c8f5-0a95-431a-9289-57ac2b8a3505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading all raw data chunks for 2014 ---\n",
      "Loaded 493308 SHARP records from 2014.\n",
      "Loaded 2299 flare records from 2014.\n",
      "\n",
      "--- Step 2: Cleaning and preparing the complete dataset ---\n",
      "\n",
      "--- Step 3: Creating labels...\n",
      "\n",
      "--- Step 4: Saving final labeled dataset ---\n",
      "\n",
      "--- Dataset Summary for 2014 ---\n",
      "Total SHARP records: 343055\n",
      "\n",
      "Binary Label Counts ('flare' column):\n",
      "flare\n",
      "0    341912\n",
      "1      1143\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Multi-class Label Counts ('classification' column):\n",
      "classification\n",
      "Non-flare    299971\n",
      "C             37117\n",
      "B              4824\n",
      "M              1127\n",
      "X                16\n",
      "Name: count, dtype: int64\n",
      "✅ Successfully saved to final_labeled_2014_dataset.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from datetime import timedelta\n",
    "import os\n",
    "\n",
    "# =========================\n",
    "# CONFIGURATION\n",
    "# =========================\n",
    "SHARP_DIR_INPUT = 'raw_sharp_data_parquet_new'\n",
    "FLARE_DIR_INPUT = 'raw_flare_data_parquet'\n",
    "FINAL_OUTPUT_FILE = 'final_labeled_2014_dataset.parquet' # Renamed for the test\n",
    "PREDICTION_WINDOW_HOURS = 24\n",
    "\n",
    "# =========================\n",
    "# 1. LOAD AND COMBINE RAW DATA (ONLY FOR 2014)\n",
    "# =========================\n",
    "print(\"--- Step 1: Loading all raw data chunks for 2014 ---\")\n",
    "# THE FIX: Added '/2014-' to glob to select only 2014 files\n",
    "sharp_files = glob.glob(f'{SHARP_DIR_INPUT}/2014-*.parquet')\n",
    "flare_files = glob.glob(f'{FLARE_DIR_INPUT}/2014-*.parquet')\n",
    "\n",
    "if not sharp_files:\n",
    "    raise FileNotFoundError(f\"No 2014 SHARP files found in '{SHARP_DIR_INPUT}'.\")\n",
    "\n",
    "sharp_df = pd.concat([pd.read_parquet(f) for f in sharp_files if os.path.getsize(f) > 0], ignore_index=True)\n",
    "print(f\"Loaded {len(sharp_df)} SHARP records from 2014.\")\n",
    "\n",
    "if flare_files:\n",
    "    flares_df = pd.concat([pd.read_parquet(f) for f in flare_files if os.path.getsize(f) > 0], ignore_index=True)\n",
    "    print(f\"Loaded {len(flares_df)} flare records from 2014.\")\n",
    "else:\n",
    "    flares_df = pd.DataFrame()\n",
    "\n",
    "# =========================\n",
    "# 2. CLEAN AND PREPARE DATA\n",
    "# =========================\n",
    "print(\"\\n--- Step 2: Cleaning and preparing the complete dataset ---\")\n",
    "sharp_df['T_REC'] = pd.to_datetime(sharp_df['T_REC'], format=\"%Y.%m.%d_%H:%M:%S_TAI\", errors='coerce')\n",
    "sharp_df.dropna(subset=['T_REC'], inplace=True)\n",
    "sharp_df.sort_values(by=['HARPNUM', 'T_REC'], inplace=True)\n",
    "sharp_df.drop_duplicates(subset=['HARPNUM', 'T_REC'], inplace=True)\n",
    "sharp_df.dropna(inplace=True) \n",
    "sharp_df['HARPNUM'] = sharp_df['HARPNUM'].astype(int)\n",
    "sharp_df['NOAA_AR'] = sharp_df['NOAA_AR'].astype(int)\n",
    "\n",
    "sharp_df['classification'] = 'Non-flare'\n",
    "sharp_df['flare'] = 0\n",
    "\n",
    "if not flares_df.empty:\n",
    "    flares_df['event_starttime'] = pd.to_datetime(flares_df['event_starttime'], errors='coerce')\n",
    "    flares_df.dropna(subset=['event_starttime', 'ar_noaanum', 'fl_goescls'], inplace=True)\n",
    "    flares_df['ar_noaanum'] = pd.to_numeric(flares_df['ar_noaanum'], errors='coerce')\n",
    "    flares_df = flares_df[flares_df['ar_noaanum'] > 0].copy()\n",
    "    flares_df['ar_noaanum'] = flares_df['ar_noaanum'].astype(int)\n",
    "\n",
    "# =========================\n",
    "# 3. CREATE LABELS\n",
    "# =========================\n",
    "if not flares_df.empty:\n",
    "    print(f\"\\n--- Step 3: Creating labels...\")\n",
    "    prediction_window = pd.Timedelta(hours=PREDICTION_WINDOW_HOURS)\n",
    "    flare_classes = ['X', 'M', 'C', 'B']\n",
    "\n",
    "    for f_class in flare_classes:\n",
    "        class_flares = flares_df[flares_df['fl_goescls'].str.startswith(f_class, na=False)]\n",
    "        for _, flare in class_flares.iterrows():\n",
    "            mask = ((sharp_df['NOAA_AR'] == flare['ar_noaanum']) &\n",
    "                    (sharp_df['T_REC'] >= flare['event_starttime'] - prediction_window) &\n",
    "                    (sharp_df['T_REC'] < flare['event_starttime']))\n",
    "            sharp_df.loc[mask, 'classification'] = f_class\n",
    "\n",
    "    sharp_df['flare'] = sharp_df['classification'].isin(['X', 'M']).astype(int)\n",
    "\n",
    "# =========================\n",
    "# 4. SAVE FINAL DATASET\n",
    "# =========================\n",
    "print(\"\\n--- Step 4: Saving final labeled dataset ---\")\n",
    "sharp_df.to_parquet(FINAL_OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"\\n--- Dataset Summary for 2014 ---\")\n",
    "print(f\"Total SHARP records: {len(sharp_df)}\")\n",
    "print(\"\\nBinary Label Counts ('flare' column):\")\n",
    "print(sharp_df['flare'].value_counts())\n",
    "print(\"\\nMulti-class Label Counts ('classification' column):\")\n",
    "print(sharp_df['classification'].value_counts())\n",
    "print(f\"✅ Successfully saved to {FINAL_OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c24f148-631f-439f-9ca7-be2d2edcf2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading all raw data chunks for 2014-2015 ---\n",
      "Loaded 918328 SHARP records from 2014-2015.\n",
      "Loaded 2299 flare records from 2014-2015.\n",
      "\n",
      "--- Step 2: Cleaning and preparing the complete dataset ---\n",
      "\n",
      "--- Step 3: Creating labels...\n",
      "\n",
      "--- Step 4: Saving final labeled dataset ---\n",
      "\n",
      "--- Dataset Summary for 2014-2015 ---\n",
      "Total SHARP records: 681458\n",
      "\n",
      "Binary Label Counts ('flare' column):\n",
      "flare\n",
      "0    673093\n",
      "1      8365\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Multi-class Label Counts ('classification' column):\n",
      "classification\n",
      "Non-flare    638374\n",
      "C             32060\n",
      "M              7269\n",
      "B              2659\n",
      "X              1096\n",
      "Name: count, dtype: int64\n",
      "✅ Successfully saved to final_labeled_2014-2015_dataset.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from datetime import timedelta\n",
    "import os\n",
    "\n",
    "# =========================\n",
    "# CONFIGURATION\n",
    "# =========================\n",
    "SHARP_DIR_INPUT = 'raw_sharp_data_parquet_new'\n",
    "FLARE_DIR_INPUT = 'raw_flare_data_parquet'\n",
    "FINAL_OUTPUT_FILE = 'final_labeled_2014-2015_dataset.parquet' # Renamed for clarity\n",
    "PREDICTION_WINDOW_HOURS = 24\n",
    "\n",
    "# =========================\n",
    "# 1. LOAD AND COMBINE RAW DATA (ONLY FOR 2014-2015)\n",
    "# =========================\n",
    "print(\"--- Step 1: Loading all raw data chunks for 2014-2015 ---\")\n",
    "\n",
    "# --- THE FIX: Select only files from 2014 and 2015 ---\n",
    "sharp_files_2014 = glob.glob(f'{SHARP_DIR_INPUT}/2014-*.parquet')\n",
    "sharp_files_2015 = glob.glob(f'{SHARP_DIR_INPUT}/2015-*.parquet')\n",
    "sharp_files = sharp_files_2014 + sharp_files_2015\n",
    "\n",
    "flare_files_2014 = glob.glob(f'{FLARE_DIR_INPUT}/2014-*.parquet')\n",
    "flare_files_2015 = glob.glob(f'{FLARE_DIR_INPUT}/2015-*.parquet')\n",
    "flare_files = flare_files_2014 + flare_files_2015\n",
    "# --------------------------------------------------------\n",
    "\n",
    "if not sharp_files:\n",
    "    raise FileNotFoundError(f\"No 2014-2015 SHARP files found in '{SHARP_DIR_INPUT}'.\")\n",
    "\n",
    "sharp_df = pd.concat([pd.read_parquet(f) for f in sharp_files if os.path.getsize(f) > 0], ignore_index=True)\n",
    "print(f\"Loaded {len(sharp_df)} SHARP records from 2014-2015.\")\n",
    "\n",
    "if flare_files:\n",
    "    flares_df = pd.concat([pd.read_parquet(f) for f in flare_files if os.path.getsize(f) > 0], ignore_index=True)\n",
    "    print(f\"Loaded {len(flares_df)} flare records from 2014-2015.\")\n",
    "else:\n",
    "    flares_df = pd.DataFrame()\n",
    "\n",
    "# =========================\n",
    "# 2. CLEAN AND PREPARE DATA\n",
    "# =========================\n",
    "print(\"\\n--- Step 2: Cleaning and preparing the complete dataset ---\")\n",
    "sharp_df['T_REC'] = pd.to_datetime(sharp_df['T_REC'], format=\"%Y.%m.%d_%H:%M:%S_TAI\", errors='coerce')\n",
    "sharp_df.dropna(subset=['T_REC'], inplace=True)\n",
    "sharp_df.sort_values(by=['HARPNUM', 'T_REC'], inplace=True)\n",
    "sharp_df.drop_duplicates(subset=['HARPNUM', 'T_REC'], inplace=True)\n",
    "sharp_df.dropna(inplace=True) \n",
    "sharp_df['HARPNUM'] = sharp_df['HARPNUM'].astype(int)\n",
    "sharp_df['NOAA_AR'] = sharp_df['NOAA_AR'].astype(int)\n",
    "\n",
    "sharp_df['classification'] = 'Non-flare'\n",
    "sharp_df['flare'] = 0\n",
    "\n",
    "if not flares_df.empty:\n",
    "    flares_df['event_starttime'] = pd.to_datetime(flares_df['event_starttime'], errors='coerce')\n",
    "    flares_df.dropna(subset=['event_starttime', 'ar_noaanum', 'fl_goescls'], inplace=True)\n",
    "    flares_df['ar_noaanum'] = pd.to_numeric(flares_df['ar_noaanum'], errors='coerce')\n",
    "    flares_df = flares_df[flares_df['ar_noaanum'] > 0].copy()\n",
    "    flares_df['ar_noaanum'] = flares_df['ar_noaanum'].astype(int)\n",
    "\n",
    "# =========================\n",
    "# 3. CREATE LABELS\n",
    "# =========================\n",
    "if not flares_df.empty:\n",
    "    print(f\"\\n--- Step 3: Creating labels...\")\n",
    "    prediction_window = pd.Timedelta(hours=PREDICTION_WINDOW_HOURS)\n",
    "    flare_classes = ['B', 'C', 'M', 'X']\n",
    "\n",
    "    for f_class in flare_classes:\n",
    "        class_flares = flares_df[flares_df['fl_goescls'].str.startswith(f_class, na=False)]\n",
    "        for _, flare in class_flares.iterrows():\n",
    "            mask = ((sharp_df['NOAA_AR'] == flare['ar_noaanum']) &\n",
    "                    (sharp_df['T_REC'] >= flare['event_starttime'] - prediction_window) &\n",
    "                    (sharp_df['T_REC'] < flare['event_starttime']))\n",
    "            sharp_df.loc[mask, 'classification'] = f_class\n",
    "\n",
    "    sharp_df['flare'] = sharp_df['classification'].isin(['X', 'M']).astype(int)\n",
    "\n",
    "# =========================\n",
    "# 4. SAVE FINAL DATASET\n",
    "# =========================\n",
    "print(\"\\n--- Step 4: Saving final labeled dataset ---\")\n",
    "sharp_df.to_parquet(FINAL_OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"\\n--- Dataset Summary for 2014-2015 ---\")\n",
    "print(f\"Total SHARP records: {len(sharp_df)}\")\n",
    "print(\"\\nBinary Label Counts ('flare' column):\")\n",
    "print(sharp_df['flare'].value_counts())\n",
    "print(\"\\nMulti-class Label Counts ('classification' column):\")\n",
    "print(sharp_df['classification'].value_counts())\n",
    "print(f\"✅ Successfully saved to {FINAL_OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ae2a528-56be-4f6c-9bf6-e0b372f4371f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading all raw data chunks for 2014-2015 ---\n",
      "Loaded 918328 SHARP records from 2014-2015.\n",
      "Loaded 2299 flare records from 2014-2015.\n",
      "\n",
      "--- Step 2: Cleaning and preparing the complete dataset ---\n",
      "\n",
      "--- Step 3: Creating labels...\n",
      "\n",
      "--- Step 4: Saving final labeled dataset ---\n",
      "\n",
      "--- Dataset Summary for 2014-2015 ---\n",
      "Total SHARP records: 681458\n",
      "\n",
      "Binary Label Counts ('flare' column):\n",
      "flare\n",
      "0    680315\n",
      "1      1143\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Multi-class Label Counts ('classification' column):\n",
      "classification\n",
      "Non-flare    638374\n",
      "C             37117\n",
      "B              4824\n",
      "M              1127\n",
      "X                16\n",
      "Name: count, dtype: int64\n",
      "✅ Successfully saved to final_labeled_2014-2015_dataset_old_logic.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from datetime import timedelta\n",
    "import os\n",
    "\n",
    "# =========================\n",
    "# CONFIGURATION\n",
    "# =========================\n",
    "SHARP_DIR_INPUT = 'raw_sharp_data_parquet_new'\n",
    "FLARE_DIR_INPUT = 'raw_flare_data_parquet'\n",
    "FINAL_OUTPUT_FILE = 'final_labeled_2014-2015_dataset_old_logic.parquet' # Renamed for clarity\n",
    "PREDICTION_WINDOW_HOURS = 24\n",
    "\n",
    "# =========================\n",
    "# 1. LOAD AND COMBINE RAW DATA (ONLY FOR 2014-2015)\n",
    "# =========================\n",
    "print(\"--- Step 1: Loading all raw data chunks for 2014-2015 ---\")\n",
    "\n",
    "# --- THE FIX: Select only files from 2014 and 2015 ---\n",
    "sharp_files_2014 = glob.glob(f'{SHARP_DIR_INPUT}/2014-*.parquet')\n",
    "sharp_files_2015 = glob.glob(f'{SHARP_DIR_INPUT}/2015-*.parquet')\n",
    "sharp_files = sharp_files_2014 + sharp_files_2015\n",
    "\n",
    "flare_files_2014 = glob.glob(f'{FLARE_DIR_INPUT}/2014-*.parquet')\n",
    "flare_files_2015 = glob.glob(f'{FLARE_DIR_INPUT}/2015-*.parquet')\n",
    "flare_files = flare_files_2014 + flare_files_2015\n",
    "# --------------------------------------------------------\n",
    "\n",
    "if not sharp_files:\n",
    "    raise FileNotFoundError(f\"No 2014-2015 SHARP files found in '{SHARP_DIR_INPUT}'.\")\n",
    "\n",
    "sharp_df = pd.concat([pd.read_parquet(f) for f in sharp_files if os.path.getsize(f) > 0], ignore_index=True)\n",
    "print(f\"Loaded {len(sharp_df)} SHARP records from 2014-2015.\")\n",
    "\n",
    "if flare_files:\n",
    "    flares_df = pd.concat([pd.read_parquet(f) for f in flare_files if os.path.getsize(f) > 0], ignore_index=True)\n",
    "    print(f\"Loaded {len(flares_df)} flare records from 2014-2015.\")\n",
    "else:\n",
    "    flares_df = pd.DataFrame()\n",
    "\n",
    "# =========================\n",
    "# 2. CLEAN AND PREPARE DATA\n",
    "# =========================\n",
    "print(\"\\n--- Step 2: Cleaning and preparing the complete dataset ---\")\n",
    "sharp_df['T_REC'] = pd.to_datetime(sharp_df['T_REC'], format=\"%Y.%m.%d_%H:%M:%S_TAI\", errors='coerce')\n",
    "sharp_df.dropna(subset=['T_REC'], inplace=True)\n",
    "sharp_df.sort_values(by=['HARPNUM', 'T_REC'], inplace=True)\n",
    "sharp_df.drop_duplicates(subset=['HARPNUM', 'T_REC'], inplace=True)\n",
    "sharp_df.dropna(inplace=True) \n",
    "sharp_df['HARPNUM'] = sharp_df['HARPNUM'].astype(int)\n",
    "sharp_df['NOAA_AR'] = sharp_df['NOAA_AR'].astype(int)\n",
    "\n",
    "sharp_df['classification'] = 'Non-flare'\n",
    "sharp_df['flare'] = 0\n",
    "\n",
    "if not flares_df.empty:\n",
    "    flares_df['event_starttime'] = pd.to_datetime(flares_df['event_starttime'], errors='coerce')\n",
    "    flares_df.dropna(subset=['event_starttime', 'ar_noaanum', 'fl_goescls'], inplace=True)\n",
    "    flares_df['ar_noaanum'] = pd.to_numeric(flares_df['ar_noaanum'], errors='coerce')\n",
    "    flares_df = flares_df[flares_df['ar_noaanum'] > 0].copy()\n",
    "    flares_df['ar_noaanum'] = flares_df['ar_noaanum'].astype(int)\n",
    "\n",
    "# =========================\n",
    "# 3. CREATE LABELS\n",
    "# =========================\n",
    "if not flares_df.empty:\n",
    "    print(f\"\\n--- Step 3: Creating labels...\")\n",
    "    prediction_window = pd.Timedelta(hours=PREDICTION_WINDOW_HOURS)\n",
    "    flare_classes = ['X','M','C','B']\n",
    "\n",
    "    for f_class in flare_classes:\n",
    "        class_flares = flares_df[flares_df['fl_goescls'].str.startswith(f_class, na=False)]\n",
    "        for _, flare in class_flares.iterrows():\n",
    "            mask = ((sharp_df['NOAA_AR'] == flare['ar_noaanum']) &\n",
    "                    (sharp_df['T_REC'] >= flare['event_starttime'] - prediction_window) &\n",
    "                    (sharp_df['T_REC'] < flare['event_starttime']))\n",
    "            sharp_df.loc[mask, 'classification'] = f_class\n",
    "\n",
    "    sharp_df['flare'] = sharp_df['classification'].isin(['X', 'M']).astype(int)\n",
    "\n",
    "# =========================\n",
    "# 4. SAVE FINAL DATASET\n",
    "# =========================\n",
    "print(\"\\n--- Step 4: Saving final labeled dataset ---\")\n",
    "sharp_df.to_parquet(FINAL_OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"\\n--- Dataset Summary for 2014-2015 ---\")\n",
    "print(f\"Total SHARP records: {len(sharp_df)}\")\n",
    "print(\"\\nBinary Label Counts ('flare' column):\")\n",
    "print(sharp_df['flare'].value_counts())\n",
    "print(\"\\nMulti-class Label Counts ('classification' column):\")\n",
    "print(sharp_df['classification'].value_counts())\n",
    "print(f\"✅ Successfully saved to {FINAL_OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8cc951-f686-42d4-be82-0cd2f67485bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
